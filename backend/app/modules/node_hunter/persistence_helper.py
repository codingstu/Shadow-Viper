# backend/app/modules/node_hunter/persistence_helper.py
# -*- coding: utf-8 -*-
"""
æŒä¹…åŒ–åŠ©æ‰‹ - è´Ÿè´£ä¸æ•°æ®åº“çš„ç¼“å­˜è¯»å†™

åŠŸèƒ½ï¼š
1. åˆå§‹åŒ–ä¸‰ä¸ªæŒä¹…åŒ–è¡¨ (sources_cache, parsed_nodes, testing_queue)
2. ç¼“å­˜è®¢é˜…æºå†…å®¹ (6å°æ—¶ TTL)
3. ç¼“å­˜è§£æåçš„èŠ‚ç‚¹ (6å°æ—¶ TTL)
4. ä¿å­˜å’Œæ¢å¤æµ‹é€Ÿé˜Ÿåˆ—è¿›åº¦
5. å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
"""

import os
import logging
import json
import base64
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import hashlib

logger = logging.getLogger(__name__)


class PersistenceHelper:
    """æŒä¹…åŒ–ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç¼“å­˜æ“ä½œ"""
    
    def __init__(self):
        self.supabase = None
        self.initialized = False
        self._init_supabase()
    
    def _init_supabase(self):
        """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
        try:
            from supabase import create_client
            
            url = os.getenv("SUPABASE_URL", "")
            key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("SUPABASE_KEY", "")
            
            if not url or not key:
                logger.warning("âš ï¸ Supabase å‡­è¯æœªé…ç½®ï¼ŒæŒä¹…åŒ–åŠŸèƒ½ç¦ç”¨")
                return
            
            self.supabase = create_client(url, key)
            logger.info("âœ… Supabase å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Supabase åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def init_persistence_tables(self):
        """åˆå§‹åŒ–æŒä¹…åŒ–è¡¨ï¼ˆä»…éœ€æ‰§è¡Œä¸€æ¬¡ï¼Œæœ‰è¶…æ—¶ä¿æŠ¤ï¼‰"""
        if not self.supabase:
            logger.warning("âš ï¸ Supabase æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¡¨åˆ›å»º")
            return
        
        try:
            logger.info("ğŸ”§ æ£€æŸ¥å¹¶åˆ›å»ºæŒä¹…åŒ–è¡¨ï¼ˆæœ€å¤š2ç§’ï¼‰...")
            
            try:
                # ğŸ”¥ åŠ å…¥ 2 ç§’è¶…æ—¶ï¼Œé˜²æ­¢ Supabase æ…¢å¯¼è‡´åç«¯å¡ä½
                async with asyncio.timeout(2):
                    # è¡¨1: sources_cache (è®¢é˜…æºç¼“å­˜)
                    await self._create_sources_cache_table()
                    
                    # è¡¨2: parsed_nodes (è§£æèŠ‚ç‚¹ç¼“å­˜)
                    await self._create_parsed_nodes_table()
                    
                    # è¡¨3: testing_queue (æµ‹é€Ÿé˜Ÿåˆ—)
                    await self._create_testing_queue_table()
                    
                    self.initialized = True
                    logger.info("âœ… æŒä¹…åŒ–è¡¨åˆå§‹åŒ–å®Œæˆ")
            except asyncio.TimeoutError:
                logger.warning("âš ï¸ Supabase å“åº”è¶…æ—¶ï¼ˆ2ç§’ï¼‰ï¼Œç»§ç»­å¯åŠ¨ï¼ˆè¡¨æ£€æŸ¥å¤±è´¥ä½†ä¸é˜»å¡åç«¯ï¼‰")
                self.initialized = False  # æ ‡è®°ä¸ºæœªåˆå§‹åŒ–ï¼Œç¨åé‡è¯•
        except Exception as e:
            logger.error(f"âŒ è¡¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼ˆç»§ç»­å¯åŠ¨ï¼‰")
    
    async def _create_sources_cache_table(self):
        """åˆ›å»ºè®¢é˜…æºç¼“å­˜è¡¨ï¼ˆå¼‚æ­¥ï¼Œé˜²æ­¢é˜»å¡ï¼‰"""
        try:
            # ğŸ”¥ æ”¹ä¸ºå¼‚æ­¥è¿è¡Œåœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œä¸é˜»å¡
            await asyncio.get_event_loop().run_in_executor(None, lambda: self.supabase.table("sources_cache").select("id").limit(1).execute())
            logger.debug("âœ… sources_cache è¡¨å·²å­˜åœ¨")
        except Exception as e:
            if "does not exist" in str(e) or "404" in str(e):
                logger.info("ğŸ“ sources_cache è¡¨ä¸å­˜åœ¨ï¼ˆéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼‰")
            else:
                logger.debug(f"âš ï¸ æ£€æŸ¥ sources_cache å¤±è´¥: {e}")
    
    async def _create_parsed_nodes_table(self):
        """åˆ›å»ºè§£æèŠ‚ç‚¹ç¼“å­˜è¡¨ï¼ˆå¼‚æ­¥ï¼Œé˜²æ­¢é˜»å¡ï¼‰"""
        try:
            # ğŸ”¥ æ”¹ä¸ºå¼‚æ­¥è¿è¡Œï¼Œä¸é˜»å¡
            await asyncio.get_event_loop().run_in_executor(None, lambda: self.supabase.table("parsed_nodes").select("id").limit(1).execute())
            logger.debug("âœ… parsed_nodes è¡¨å·²å­˜åœ¨")
        except Exception as e:
            if "does not exist" in str(e) or "404" in str(e):
                logger.info("ğŸ“ parsed_nodes è¡¨ä¸å­˜åœ¨ï¼ˆéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼‰")
            else:
                logger.debug(f"âš ï¸ æ£€æŸ¥ parsed_nodes å¤±è´¥: {e}")
            else:
                raise e
    
    async def _create_testing_queue_table(self):
        """åˆ›å»ºæµ‹é€Ÿé˜Ÿåˆ—è¡¨ï¼ˆå¼‚æ­¥ï¼Œé˜²æ­¢é˜»å¡ï¼‰"""
        try:
            # ğŸ”¥ æ”¹ä¸ºå¼‚æ­¥è¿è¡Œï¼Œä¸é˜»å¡
            await asyncio.get_event_loop().run_in_executor(None, lambda: self.supabase.table("testing_queue").select("id").limit(1).execute())
            logger.debug("âœ… testing_queue è¡¨å·²å­˜åœ¨")
        except Exception as e:
            if "does not exist" in str(e) or "404" in str(e):
                logger.info("ğŸ“ testing_queue è¡¨ä¸å­˜åœ¨ï¼ˆéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼‰")
                # CREATE TABLE testing_queue (
                #   id BIGINT PRIMARY KEY,
                #   group_number INT,
                #   group_position INT,
                #   node_host VARCHAR(255),
                #   node_port INT,
                #   node_name VARCHAR(255),
                #   status VARCHAR(20),
                #   attempted_count INT DEFAULT 0,
                #   last_tested_at TIMESTAMP,
                #   created_at TIMESTAMP,
                #   updated_at TIMESTAMP
                # )
            else:
                raise e
    
    # ==================== è®¢é˜…æºç¼“å­˜ ====================
    
    async def save_sources_cache(self, sources: List[str], node_contents: Dict[str, List[str]]) -> bool:
        """
        ä¿å­˜è®¢é˜…æºå’Œçˆ¬å–å†…å®¹åˆ°ç¼“å­˜
        
        Args:
            sources: è®¢é˜…æº URL åˆ—è¡¨
            node_contents: æºURL -> èŠ‚ç‚¹åˆ—è¡¨çš„æ˜ å°„
        """
        if not self.supabase:
            return False
        
        try:
            for source_url in sources:
                nodes = node_contents.get(source_url, [])
                if not nodes:
                    continue
                
                # å‹ç¼©å†…å®¹å¹¶ base64 ç¼–ç 
                content_str = json.dumps(nodes, ensure_ascii=False)
                content_b64 = base64.b64encode(content_str.encode()).decode()
                
                # ç”Ÿæˆå”¯ä¸€ ID
                content_hash = hashlib.md5(content_str.encode()).hexdigest()
                record_id = int(content_hash[:8], 16)
                
                record = {
                    "id": record_id,
                    "source_url": source_url[:500],
                    "content": content_b64,
                    "node_count": len(nodes),
                    "last_fetched_at": datetime.utcnow().isoformat(),
                    "ttl_hours": 6,
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }
                
                # upsert (å¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥)
                self.supabase.table("sources_cache").upsert(record).execute()
            
            logger.info(f"âœ… å·²ç¼“å­˜ {len(sources)} ä¸ªè®¢é˜…æº")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æºç¼“å­˜å¤±è´¥: {e}")
            return False
    
    async def load_sources_cache(self, sources: List[str]) -> Dict[str, List[str]]:
        """
        ä»ç¼“å­˜åŠ è½½è®¢é˜…æºå†…å®¹
        
        Returns:
            æºURL -> èŠ‚ç‚¹åˆ—è¡¨çš„æ˜ å°„
        """
        if not self.supabase:
            return {}
        
        try:
            result = {}
            for source_url in sources:
                # å°è¯•ä»ç¼“å­˜æŸ¥è¯¢
                response = self.supabase.table("sources_cache")\
                    .select("*")\
                    .eq("source_url", source_url[:500])\
                    .execute()
                
                if not response.data:
                    continue
                
                record = response.data[0]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                last_fetched = datetime.fromisoformat(record["last_fetched_at"])
                ttl_hours = record.get("ttl_hours", 6)
                
                if datetime.utcnow() - last_fetched > timedelta(hours=ttl_hours):
                    logger.debug(f"â° æºç¼“å­˜å·²è¿‡æœŸ: {source_url[:30]}")
                    continue
                
                # è§£ç å†…å®¹
                try:
                    content_str = base64.b64decode(record["content"]).decode()
                    nodes = json.loads(content_str)
                    result[source_url] = nodes
                    logger.debug(f"âœ… åŠ è½½ç¼“å­˜æº: {source_url[:30]} ({len(nodes)} ä¸ªèŠ‚ç‚¹)")
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ç ç¼“å­˜å¤±è´¥: {e}")
            
            return result
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æºç¼“å­˜å¤±è´¥: {e}")
            return {}
    
    # ==================== è§£æèŠ‚ç‚¹ç¼“å­˜ ====================
    
    async def save_parsed_nodes(self, nodes: List[Dict]) -> bool:
        """ä¿å­˜è§£æåçš„èŠ‚ç‚¹åˆ°ç¼“å­˜"""
        if not self.supabase:
            return False
        
        try:
            records = []
            for node in nodes:
                record = {
                    "id": int(hashlib.md5(
                        f"{node.get('host')}:{node.get('port')}".encode()
                    ).hexdigest()[:8], 16),
                    "host": node.get("host", ""),
                    "port": node.get("port", 0),
                    "name": node.get("name", "")[:255],
                    "protocol": node.get("protocol", "")[:50],
                    "full_content": json.dumps(node, ensure_ascii=False),
                    "source_url": node.get("source_url", "")[:500],
                    "parsed_at": datetime.utcnow().isoformat(),
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }
                records.append(record)
            
            # æ‰¹é‡ upsert
            if records:
                self.supabase.table("parsed_nodes").upsert(records).execute()
                logger.info(f"âœ… å·²ç¼“å­˜ {len(records)} ä¸ªè§£æèŠ‚ç‚¹")
            
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜èŠ‚ç‚¹ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    async def load_parsed_nodes(self) -> List[Dict]:
        """ä»ç¼“å­˜åŠ è½½è§£æèŠ‚ç‚¹"""
        if not self.supabase:
            return []
        
        try:
            # æŸ¥è¯¢æœ€è¿‘ 6 å°æ—¶å†…çš„èŠ‚ç‚¹
            six_hours_ago = (datetime.utcnow() - timedelta(hours=6)).isoformat()
            
            response = self.supabase.table("parsed_nodes")\
                .select("full_content")\
                .gte("updated_at", six_hours_ago)\
                .order("updated_at", desc=True)\
                .execute()
            
            nodes = []
            for record in response.data:
                try:
                    node = json.loads(record["full_content"])
                    nodes.append(node)
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£æèŠ‚ç‚¹å¤±è´¥: {e}")
            
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½ {len(nodes)} ä¸ªè§£æèŠ‚ç‚¹")
            return nodes
        except Exception as e:
            logger.error(f"âŒ åŠ è½½èŠ‚ç‚¹ç¼“å­˜å¤±è´¥: {e}")
            return []
    
    # ==================== æµ‹é€Ÿé˜Ÿåˆ— ====================
    
    async def save_testing_queue(self, queue_tasks: List[Dict]) -> bool:
        """ä¿å­˜æµ‹é€Ÿé˜Ÿåˆ—ä»»åŠ¡"""
        if not self.supabase:
            return False
        
        try:
            records = []
            for idx, task in enumerate(queue_tasks):
                record = {
                    "id": int(hashlib.md5(
                        f"{task['node_host']}:{task['node_port']}:{idx}".encode()
                    ).hexdigest()[:8], 16),
                    "group_number": task.get("group_number", 0),
                    "group_position": task.get("group_position", 0),
                    "node_host": task.get("node_host", ""),
                    "node_port": task.get("node_port", 0),
                    "node_name": task.get("node_name", "")[:255],
                    "status": task.get("status", "pending"),
                    "attempted_count": task.get("attempted_count", 0),
                    "last_tested_at": task.get("last_tested_at"),
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }
                records.append(record)
            
            if records:
                self.supabase.table("testing_queue").upsert(records).execute()
                logger.debug(f"âœ… å·²ä¿å­˜ {len(records)} ä¸ªé˜Ÿåˆ—ä»»åŠ¡")
            
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é˜Ÿåˆ—å¤±è´¥: {e}")
            return False
    
    async def load_testing_queue(self) -> List[Dict]:
        """åŠ è½½æœªå®Œæˆçš„æµ‹é€Ÿé˜Ÿåˆ—"""
        if not self.supabase:
            return []
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ŒæŒ‰ç»„å’Œä½ç½®æ’åº
            response = self.supabase.table("testing_queue")\
                .select("*")\
                .neq("status", "completed")\
                .order("group_number", desc=False)\
                .order("group_position", desc=False)\
                .execute()
            
            if response.data:
                logger.info(f"âœ… æ¢å¤ {len(response.data)} ä¸ªæœªå®Œæˆçš„é˜Ÿåˆ—ä»»åŠ¡")
                return response.data
            
            return []
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½é˜Ÿåˆ—å¤±è´¥: {e}")
            return []
    
    async def update_task_status(self, node_host: str, node_port: int, status: str) -> bool:
        """æ›´æ–°å•ä¸ªä»»åŠ¡çŠ¶æ€"""
        if not self.supabase:
            return False
        
        try:
            self.supabase.table("testing_queue")\
                .update({
                    "status": status,
                    "last_tested_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                })\
                .eq("node_host", node_host)\
                .eq("node_port", node_port)\
                .execute()
            
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    # ==================== æ•°æ®æ¸…ç† ====================
    
    async def cleanup_expired_cache(self) -> bool:
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ•°æ®"""
        if not self.supabase:
            return False
        
        try:
            # åˆ é™¤ 7 å¤©å‰çš„å·²å®Œæˆä»»åŠ¡
            seven_days_ago = (datetime.utcnow() - timedelta(days=7)).isoformat()
            self.supabase.table("testing_queue")\
                .delete()\
                .eq("status", "completed")\
                .lt("created_at", seven_days_ago)\
                .execute()
            
            # åˆ é™¤è¿‡æœŸçš„æºç¼“å­˜ (> 24å°æ—¶)
            twentyfour_hours_ago = (datetime.utcnow() - timedelta(hours=24)).isoformat()
            self.supabase.table("sources_cache")\
                .delete()\
                .lt("last_fetched_at", twentyfour_hours_ago)\
                .execute()
            
            logger.info("âœ… è¿‡æœŸç¼“å­˜æ¸…ç†å®Œæˆ")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            return False


# å…¨å±€æŒä¹…åŒ–ç®¡ç†å™¨å®ä¾‹
_persistence_instance = None


def get_persistence() -> PersistenceHelper:
    """è·å–å…¨å±€æŒä¹…åŒ–ç®¡ç†å™¨å®ä¾‹"""
    global _persistence_instance
    if _persistence_instance is None:
        _persistence_instance = PersistenceHelper()
    return _persistence_instance
