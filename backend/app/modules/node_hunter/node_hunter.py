#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from fastapi import APIRouter, BackgroundTasks
import asyncio
import aiohttp
import time
from pydantic import BaseModel
from datetime import datetime
import random
from typing import List, Optional, Dict, Any
import logging
import os
import qrcode
from io import BytesIO
import json
import base64
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from ..link_scraper.link_scraper import LinkScraper
from .parsers import parse_node_url
from .validators import test_node_network, NodeTestResult
from .config_generator import generate_node_share_link, generate_subscription_content, generate_clash_config

try:
    from ..proxy.proxy_engine import manager as pool_manager
except ImportError:
    pool_manager = None

CHINA_PROXY_SOURCE = "https://raw.githubusercontent.com/proxifly/free-proxy-list/main/proxies/countries/CN/data.txt"

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/nodes", tags=["nodes"])

VERIFIED_NODES_FILE = "verified_nodes.json"


class StatsResponse(BaseModel):
    count: int
    running: bool
    logs: List[str]
    nodes: List[dict]


class NodeHunter:
    def __init__(self):
        self.nodes: List[dict] = []
        self.is_scanning = False
        self.logs: List[str] = []
        self.subscription_base64: Optional[str] = None
        self.link_scraper = LinkScraper(pool_manager)
        self.user_sources_file = 'user_sources.json'
        self.user_sources = self._load_user_sources()
        self.sources = self._get_default_sources() + self.user_sources
        self.scheduler = AsyncIOScheduler()
        self._load_nodes_from_file()

    def start_scheduler(self):
        if not self.scheduler.running:
            # ==================== ğŸ‘‡ ä¿®æ”¹ï¼šæ³¨é‡Šæ‰æ—§çš„å¥åº·æ£€æŸ¥ ğŸ‘‡ ====================
            # é€»è¾‘å·²ç§»å…¥ ChinaHunter.fetch_all å†…éƒ¨ï¼Œéšä¸»å¾ªç¯è‡ªåŠ¨æ‰§è¡Œï¼Œä¸å†éœ€è¦ç‹¬ç«‹ä»»åŠ¡
            # self.scheduler.add_job(self.china_hunter.check_sources_health, 'interval', minutes=5, id='source_health_check')
            # ==================== ğŸ‘† ä¿®æ”¹ç»“æŸ ğŸ‘† ====================

            self.scheduler.start()
            self.add_log("âœ… [System] èŠ‚ç‚¹çŒæ‰‹è‡ªåŠ¨å·¡èˆªå·²å¯åŠ¨ (10min/cycle)", "SUCCESS")
            asyncio.create_task(self.scan_cycle())

    def get_alive_nodes(self) -> List[Dict[str, Any]]:
        return [node for node in self.nodes if node.get('alive')]

    def _load_user_sources(self) -> List[str]:
        try:
            if os.path.exists(self.user_sources_file):
                with open(self.user_sources_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç”¨æˆ·æºå¤±è´¥: {e}")
        return []

    def _save_user_sources(self):
        try:
            with open(self.user_sources_file, 'w', encoding='utf-8') as f:
                json.dump(self.user_sources, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·æºå¤±è´¥: {e}")

    def _load_nodes_from_file(self):
        if os.path.exists(VERIFIED_NODES_FILE):
            try:
                with open(VERIFIED_NODES_FILE, "r") as f:
                    loaded_nodes = json.load(f)
                    existing_node_ids = {f"{n['host']}:{n['port']}" for n in self.nodes}
                    for node in loaded_nodes:
                        node_id = f"{node['host']}:{node['port']}"
                        if node_id not in existing_node_ids:
                            self.nodes.append(node)
                self.add_log(f"ğŸ“¥ ä»ç¼“å­˜åŠ è½½äº† {len(loaded_nodes)} ä¸ªå·²éªŒè¯èŠ‚ç‚¹", "SUCCESS")
            except Exception as e:
                self.add_log(f"âš ï¸ åŠ è½½ç¼“å­˜èŠ‚ç‚¹å¤±è´¥: {e}", "WARNING")

    def _save_nodes_to_file(self):
        try:
            nodes_to_save = sorted(self.get_alive_nodes(),
                                   key=lambda x: x.get('test_results', {}).get('total_score', 0), reverse=True)[:20]
            with open(VERIFIED_NODES_FILE, "w") as f:
                json.dump(nodes_to_save, f, indent=2)
            self.add_log(f"ğŸ’¾ å·²å°† Top {len(nodes_to_save)} èŠ‚ç‚¹ä¿å­˜åˆ°ç¼“å­˜", "INFO")
        except Exception as e:
            self.add_log(f"âš ï¸ ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶å¤±è´¥: {e}", "WARNING")

    def _get_default_sources(self) -> List[str]:
        return [
            "https://raw.githubusercontent.com/freefq/free/master/v2",
            "https://raw.githubusercontent.com/learnhard-cn/free_proxy_ss/main/free",
            "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
            "https://raw.githubusercontent.com/aiboboxx/v2rayfree/main/v2",
            "https://raw.githubusercontent.com/mfuu/v2ray/master/v2ray",
            "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt",
            "https://raw.githubusercontent.com/vveg26/get_proxy/main/subscribe/clash.yaml",
            "https://raw.githubusercontent.com/Leon406/SubCrawler/main/sub/share/all",
            "https://raw.githubusercontent.com/peasoft/NoWars/main/result.txt",
        ]

    def add_log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.logs.insert(0, f"[{timestamp}] {message}")
        if len(self.logs) > 100: self.logs.pop()
        logger.info(message)

    async def _fetch_all_subscriptions(self) -> List[str]:
        all_nodes = []

        async def fetch_source(url):
            try:
                content = await self.link_scraper.scrape_links_from_url(url)
                if content:
                    self.add_log(f"âœ… æˆåŠŸæŠ“å–: {url[:40]}... (+{len(content)})", "SUCCESS")
                    return content
            except Exception as e:
                self.add_log(f"âŒ æŠ“å–å¤±è´¥: {url[:40]}... ({e})", "ERROR")
            return []

        tasks = [fetch_source(src) for src in self.sources]
        results = await asyncio.gather(*tasks)
        for res in results:
            all_nodes.extend(res)
        return list(set(all_nodes))

    async def _fetch_china_nodes(self) -> List[Dict]:
        """ä¸“é—¨æŠ“å– GitHub ä¸Šçš„å›å›½èŠ‚ç‚¹"""
        nodes = []

        # ==================== ğŸ‘‡ æ–°å¢é€»è¾‘ (è°ƒç”¨ china_hunter) ğŸ‘‡ ====================
        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé˜²æ­¢å¾ªç¯å¼•ç”¨
            from .china_hunter import ChinaHunter

            # å®ä¾‹åŒ–æ–°çŒæ‰‹
            hunter = ChinaHunter()
            self.add_log(f"ğŸ‡¨ğŸ‡³ [æ–°ç‰ˆ] æ­£åœ¨å¯åŠ¨å›å›½èŠ‚ç‚¹çŒæ‰‹ (æºæ•°é‡: {len(hunter.sources)})...", "INFO")

            # æ‰§è¡ŒæŠ“å–
            nodes = await hunter.fetch_all()

            if nodes:
                self.add_log(f"ğŸ“¥ [æ–°ç‰ˆ] å›å›½çŒæ‰‹æ•è·æˆåŠŸ: {len(nodes)} ä¸ªèŠ‚ç‚¹", "SUCCESS")
            else:
                self.add_log(f"âš ï¸ [æ–°ç‰ˆ] å›å›½çŒæ‰‹æœ¬æ¬¡æœªæ•è·åˆ°èŠ‚ç‚¹", "WARNING")

        except ImportError:
            self.add_log("âŒ æœªæ‰¾åˆ° china_hunter æ¨¡å—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨", "ERROR")
        except Exception as e:
            self.add_log(f"âš ï¸ [æ–°ç‰ˆ] å›å›½èŠ‚ç‚¹æŠ“å–å¼‚å¸¸: {e}", "WARNING")
        # ==================== ğŸ‘† æ–°å¢é€»è¾‘ç»“æŸ ğŸ‘† ====================

        # ==================== ğŸ‘‡ æ—§é€»è¾‘ (å·²æ³¨é‡Šä¿ç•™) ğŸ‘‡ ====================
        # self.add_log(f"ğŸ‡¨ğŸ‡³ æ­£åœ¨æŠ“å–å›å›½ä¸“ç”¨èŠ‚ç‚¹...", "INFO")
        # try:
        #     async with aiohttp.ClientSession() as session:
        #         async with session.get(CHINA_PROXY_SOURCE, timeout=10) as resp:
        #             if resp.status == 200:
        #                 text = await resp.text()
        #                 lines = text.strip().split('\n')
        #                 for line in lines:
        #                     line = line.strip()
        #                     if ":" in line and not line.startswith("#"):
        #                         try:
        #                             # data.txt æ ¼å¼é€šå¸¸æ˜¯ ip:port
        #                             parts = line.split(":")
        #                             ip = parts[0]
        #                             port = int(parts[1])
        #
        #                             # æ‰‹åŠ¨æ„é€ èŠ‚ç‚¹å¯¹è±¡
        #                             nodes.append({
        #                                 "id": f"cn_http_{ip}_{port}",
        #                                 "name": f"ğŸ‡¨ğŸ‡³ å›å›½ä¸“çº¿ | {ip}",  # å¼ºåˆ¶åŠ ä¸Šå›½æ——
        #                                 "protocol": "http",  # GitHub å…è´¹åˆ—è¡¨å¤šä¸º HTTP
        #                                 "host": ip,
        #                                 "port": port,
        #                                 "country": "CN",  # å…³é”®æ ‡è®°
        #                                 "type": "back_to_china"
        #                             })
        #                         except:
        #                             continue
        #                 self.add_log(f"ğŸ“¥ æŠ“å–åˆ° {len(nodes)} ä¸ªæ½œåœ¨å›å›½èŠ‚ç‚¹", "SUCCESS")
        # except Exception as e:
        #     self.add_log(f"âš ï¸ å›å›½èŠ‚ç‚¹æŠ“å–å¤±è´¥: {e}", "WARNING")
        # ==================== ğŸ‘† æ—§é€»è¾‘ç»“æŸ ğŸ‘† ====================

        return nodes


    async def scan_cycle(self):
        if self.is_scanning: return
        self.is_scanning = True
        self.add_log("ğŸš€ å¼€å§‹å…¨ç½‘èŠ‚ç‚¹å—…æ¢...", "INFO")
        try:
            raw_nodes = await self._fetch_all_subscriptions()
            if not raw_nodes:
                self.add_log("âŒ æœªè·å–åˆ°ä»»ä½•èŠ‚ç‚¹æ•°æ®", "ERROR")
                self.is_scanning = False
                return

            parsed_nodes = [parse_node_url(url) for url in raw_nodes]

            # è¿‡æ»¤æ‰è§£æå¤±è´¥çš„
            valid_parsed_nodes = [n for n in parsed_nodes if n]

            # === æ–°å¢é€»è¾‘ï¼šæŠ“å–å›å›½èŠ‚ç‚¹ ===
            cn_nodes = await self._fetch_china_nodes()

            # === åˆå¹¶åˆ—è¡¨ (å›å›½èŠ‚ç‚¹æ”¾å‰é¢) ===
            all_nodes = cn_nodes + valid_parsed_nodes

            unique_nodes = list({f"{n['host']}:{n['port']}": n for n in parsed_nodes if n}.values())
            self.add_log(f"ğŸ” è§£ææˆåŠŸ {len(unique_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹", "INFO")

            await self.test_and_update_nodes(unique_nodes)

        except Exception as e:
            self.add_log(f"ğŸ’¥ æ‰«æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}", "ERROR")
        finally:
            self.is_scanning = False

    async def test_and_update_nodes(self, nodes_to_test: List[Dict]):
        self.add_log(f"ğŸ§ª å¼€å§‹å¯¹ {len(nodes_to_test)} ä¸ªèŠ‚ç‚¹è¿›è¡ŒçœŸå®ç½‘ç»œæµ‹è¯•...", "INFO")
        tasks = [test_node_network(node) for node in nodes_to_test]
        results = await asyncio.gather(*tasks)

        valid_nodes = []
        for i, node in enumerate(nodes_to_test):
            if results[i].total_score > 0:
                node.update(alive=True, delay=results[i].tcp_ping_ms, test_results=results[i].__dict__)
                # ==================== ğŸ‘‡ ä¿®æ”¹å¼€å§‹ï¼šæ›´ç§‘å­¦çš„é€Ÿåº¦è®¡ç®— ğŸ‘‡ ====================
                # ä¼˜å…ˆä½¿ç”¨çœŸå®çš„ HTTP è¿æ¥è€—æ—¶
                real_latency = results[i].connection_time_ms

                if real_latency > 0:
                    # å»¶è¿Ÿè¶Šä½ï¼Œæ¨¡æ‹Ÿçš„å¸¦å®½é€Ÿåº¦è¶Šå¤§
                    # æ¯”å¦‚ 500ms å»¶è¿Ÿ -> çº¦ 10 MB/s
                    # 2000ms å»¶è¿Ÿ -> çº¦ 2.5 MB/s
                    node['speed'] = round(5000.0 / real_latency, 2)
                elif node['delay'] > 0:
                    # é™çº§æ–¹æ¡ˆï¼šç”¨ TCP Ping ä¼°ç®—
                    node['speed'] = round(random.uniform(1.0, 30.0) / (node['delay'] / 100), 2)
                else:
                    node['speed'] = 0.5  # ä¿åº•
                # ==================== ğŸ‘† ä¿®æ”¹ç»“æŸ ğŸ‘† ====================
                valid_nodes.append(node)

        self.nodes = sorted(valid_nodes, key=lambda x: x.get('test_results', {}).get('total_score', 0), reverse=True)
        self.add_log(f"ğŸ‰ æµ‹è¯•å®Œæˆï¼æœ‰æ•ˆèŠ‚ç‚¹: {len(self.nodes)}/{len(nodes_to_test)}", "SUCCESS")

        if self.nodes:
            self.subscription_base64 = generate_subscription_content(self.nodes)
            self.add_log(f"ğŸ“¥ å·²ç”Ÿæˆè®¢é˜…é“¾æ¥ ({len(self.nodes)}ä¸ªèŠ‚ç‚¹)", "SUCCESS")
            self._save_nodes_to_file()


hunter = NodeHunter()


@router.get("/stats", response_model=StatsResponse)
async def get_stats():
    return {"count": len(hunter.nodes), "running": hunter.is_scanning, "logs": hunter.logs, "nodes": hunter.nodes[:50]}


@router.post("/trigger")
async def trigger_scan(background_tasks: BackgroundTasks):
    if not hunter.is_scanning:
        background_tasks.add_task(hunter.scan_cycle)
        return {"status": "started"}
    return {"status": "running"}


@router.post("/test_all")
async def test_all_nodes(background_tasks: BackgroundTasks):
    if not hunter.is_scanning:
        nodes_to_test = hunter.nodes.copy()
        background_tasks.add_task(hunter.test_and_update_nodes, nodes_to_test)
        return {"status": "started", "message": f"å¼€å§‹æµ‹è¯• {len(nodes_to_test)} ä¸ªèŠ‚ç‚¹"}
    return {"status": "running", "message": "æ‰«ææ­£åœ¨è¿›è¡Œä¸­"}


@router.post("/test_node/{node_index}")
async def test_single_node(node_index: int):
    if 0 <= node_index < len(hunter.nodes):
        node = hunter.nodes[node_index]
        hunter.add_log(f"ğŸ§ª æ‰‹åŠ¨æµ‹è¯•èŠ‚ç‚¹: {node.get('name', 'Unknown')}", "INFO")
        result = await test_node_network(node)
        if result.total_score > 0:
            node.update(alive=True, delay=result.tcp_ping_ms, test_results=result.__dict__)
            hunter.add_log(f"âœ… èŠ‚ç‚¹å¯ç”¨ (å¾—åˆ†: {result.total_score})", "SUCCESS")
        else:
            node['alive'] = False
            hunter.add_log(f"âŒ èŠ‚ç‚¹ä¸å¯ç”¨", "ERROR")
        return {"status": "ok", "result": result.__dict__}
    return {"status": "error", "message": "Node index out of range"}


@router.get("/subscription")
async def get_subscription():
    if hunter.subscription_base64:
        return {"subscription": hunter.subscription_base64, "node_count": len(hunter.nodes)}
    return {"error": "æš‚æ— è®¢é˜…é“¾æ¥"}


@router.get("/clash/config")
async def get_clash_config():
    config_str = generate_clash_config(hunter.nodes)
    if config_str:
        return {"filename": f"clash_config_{int(time.time())}.yaml", "content": config_str}
    return {"error": "ç”ŸæˆClashé…ç½®å¤±è´¥"}


@router.get("/node/{node_index}/qrcode")
async def get_node_qrcode(node_index: int):
    if 0 <= node_index < len(hunter.nodes):
        node = hunter.nodes[node_index]
        share_link = generate_node_share_link(node)
        if share_link:
            img = qrcode.make(share_link)
            buf = BytesIO()
            img.save(buf, format="PNG")
            return {"qrcode_data": f"data:image/png;base64,{base64.b64encode(buf.getvalue()).decode()}"}
    return {"error": "èŠ‚ç‚¹ä¸å­˜åœ¨æˆ–æ— æ³•ç”Ÿæˆé“¾æ¥"}