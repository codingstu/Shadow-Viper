# backend/app/modules/node_hunter/china_hunter.py
import asyncio
import aiohttp
import re
import logging
import base64
import yaml
from typing import List, Dict, Any, Tuple

try:
    from .parsers import parse_node_url
except ImportError:
    from parsers import parse_node_url

logger = logging.getLogger(__name__)


class ChinaHunter:
    """
    ðŸ‡¨ðŸ‡³ æ™ºèƒ½å›žå›½èŠ‚ç‚¹çŒŽæ‰‹ (V4: TG/Discord/Twitter å…¨ç½‘èšåˆç‰ˆ)
    ç‰¹æ€§ï¼š
    1. æŽ¥å…¥ TelegramV2rayCollector & LalatinaHub (æ•°ä¸‡èŠ‚ç‚¹çš„å¤§æ± å­)ã€‚
    2. å¢žå¼ºåž‹å…³é”®å­—è¿‡æ»¤ï¼Œè¦†ç›–ä¸­å›½ä¸»æµåŸŽå¸‚å’Œäº‘åŽ‚å•†ã€‚
    3. æ™ºèƒ½è§£æžæ··åˆæ ¼å¼ (Base64, YAML, Text)ã€‚
    """

    def __init__(self):
        self.scan_cycle_count = 0
        self.source_stats = {}

        # ðŸŽ¯ å…³é”®å­—è¿‡æ»¤å™¨ï¼šå¤§å¹…æ‰©å……å›½å†…åŸŽå¸‚å’Œè¿è¥å•†
        self.cn_keywords = [
            "å›žå›½", "back", "CN", "China", "ä¸­å›½",
            "ä¸Šæµ·", "åŒ—äº¬", "æ­å·ž", "æ·±åœ³", "å¹¿å·ž", "æˆéƒ½", "æ­¦æ±‰", "å¤©æ´¥", "é‡åº†", "å—äº¬", "é•¿æ²™", "è‹å·ž",
            "Shanghai", "Beijing", "Shenzhen", "Hangzhou", "Guangzhou", "Chengdu", "Wuhan",
            "Aliyun", "Tencent", "Huawei", "Qcloud", "BGP", "CT", "CU", "CM",  # è¿è¥å•†/äº‘åŽ‚å•†
            "æ±Ÿè‹", "æµ™æ±Ÿ", "å¹¿ä¸œ", "å››å·", "å±±ä¸œ"
        ]

        self.sources = [
            # === ðŸ‘‘ ç¥žçº§èšåˆ (ä¸“é—¨çˆ¬å– TG/Discord/Twitter) ===
            "https://raw.githubusercontent.com/yebekhe/TelegramV2rayCollector/main/sub/mix",
            "https://raw.githubusercontent.com/LalatinaHub/Mineral/master/result/nodes",
            "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/EternityAir",

            # === ðŸ”µ Telegram è®¢é˜…æº (Clash/YAML æ ¼å¼) ===
            "https://raw.githubusercontent.com/vveg26/get_proxy/main/dist/clash.config.yaml",
            "https://raw.githubusercontent.com/anaer/Sub/main/clash.yaml",
            "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml",
            "https://raw.githubusercontent.com/juewuy/ShellClash/master/public/public.yaml",

            # === ðŸŸ  ç»å…¸è®¢é˜…æº (Base64/Txt) ===
            "https://raw.githubusercontent.com/aiboboxx/v2rayfree/main/v2",
            "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
            "https://raw.githubusercontent.com/mfuu/v2ray/master/v2ray",
            "https://raw.githubusercontent.com/learnhard-cn/free_proxy_ss/main/free",
            "https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/v2ray.config.txt",

            # === ðŸŸ¢ ä¸“ç²¾ CN/IP ç›´è¿žåˆ—è¡¨ (HTTP/SOCKS5) ===
            "https://raw.githubusercontent.com/proxifly/free-proxy-list/main/proxies/countries/CN/data.txt",
            "https://raw.githubusercontent.com/juepile/Proxy-List/main/China.txt",
            "https://raw.githubusercontent.com/list-404/CN-Proxy/main/http.txt",
            "https://raw.githubusercontent.com/peasoft/NoWars/main/result.txt",
            # ðŸ”¥ æ–°å¢žï¼šæ›´å¤š HTTP/SOCKS5 æº
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/SOCKS5_RAW.txt",
        ]

        # åˆå§‹åŒ–çŠ¶æ€
        for src in self.sources:
            self.source_stats[src] = {"is_disabled": False, "disabled_at": 0, "retry_fails": 0}

    async def fetch_all(self) -> List[Dict[str, Any]]:
        self.scan_cycle_count += 1
        current_cycle = self.scan_cycle_count

        target_urls = []
        for url in list(self.sources):
            stats = self.source_stats.get(url, {"is_disabled": False, "disabled_at": 0, "retry_fails": 0})
            if stats["is_disabled"]:
                if (current_cycle - stats["disabled_at"]) >= 10: target_urls.append(url)
            else:
                target_urls.append(url)

        if not target_urls: return []

        logger.info(f"ðŸ‡¨ðŸ‡³ [å…¨ç½‘çŒŽæ‰‹] æ‰«æ {len(target_urls)} ä¸ªèšåˆæº (å«TG/Discordé‡‡é›†åº“)...")

        tasks = [self._fetch_url(url) for url in target_urls]
        results = await asyncio.gather(*tasks)

        merged_nodes = []
        seen = set()

        for i, (nodes, is_success) in enumerate(results):
            url = target_urls[i]
            if url not in self.source_stats: self.source_stats[url] = {"is_disabled": False, "disabled_at": 0,
                                                                       "retry_fails": 0}
            stats = self.source_stats[url]

            if is_success:
                if stats["is_disabled"]:
                    stats["is_disabled"] = False
                    stats["retry_fails"] = 0

                for node in nodes:
                    # ðŸ” æ ¸å¿ƒç­›é€‰
                    if self._is_cn_node(node, url):
                        # å¼ºåˆ¶åŠ ä¸Šå›½æ——
                        if "ðŸ‡¨ðŸ‡³" not in node.get('name', ''):
                            node['name'] = f"ðŸ‡¨ðŸ‡³ {node.get('name')}"

                        node['country'] = 'CN'
                        node['type'] = 'back_to_china'

                        unique_id = f"{node['host']}:{node['port']}"
                        if unique_id not in seen:
                            seen.add(unique_id)
                            merged_nodes.append(node)
            else:
                if not stats["is_disabled"]:
                    stats["is_disabled"] = True
                    stats["disabled_at"] = current_cycle

        logger.info(f"ðŸ‡¨ðŸ‡³ [å…¨ç½‘çŒŽæ‰‹] ç»æ·±åº¦ç­›é€‰ï¼Œæ•èŽ· {len(merged_nodes)} ä¸ªå›žå›½èŠ‚ç‚¹")
        return merged_nodes

    def _is_cn_node(self, node: Dict, source_url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå›žå›½èŠ‚ç‚¹"""
        # ðŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„åˆ¤æ–­ï¼Œé˜²æ­¢è¯¯ä¼¤
        # 1. ä¸“ç²¾æºç›´é€š
        if "CN" in source_url or "China" in source_url or "cn-proxies" in source_url:
            return True

        # 2. å…³é”®å­—åŒ¹é… (ä¸åŒºåˆ†å¤§å°å†™)
        name = node.get('name', '').upper()
        
        # æ˜Žç¡®çš„å›žå›½å…³é”®å­—
        back_home_keywords = ["å›žå›½", "BACK"]
        for kw in back_home_keywords:
            if kw in name:
                return True

        # åŸŽå¸‚å’Œè¿è¥å•†å…³é”®å­— (ä»…å½“æ²¡æœ‰æ˜Žç¡®çš„â€œå‡ºå¢ƒâ€æ ‡å¿—æ—¶)
        exit_keywords = ["US", "JP", "HK", "SG", "TW", "KR", "å‡º", "->"]
        has_exit_keyword = any(kw in name for kw in exit_keywords)
        
        if not has_exit_keyword:
            for kw in self.cn_keywords:
                if kw.upper() in name:
                    return True

        return False

    async def _fetch_url(self, url: str) -> Tuple[List[Dict[str, Any]], bool]:
        nodes = []
        try:
            # å¢žåŠ è¶…æ—¶ï¼Œå› ä¸º TelegramV2rayCollector æ–‡ä»¶å¾ˆå¤§
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as resp:
                    if resp.status != 200: return [], False
                    text = await resp.text()

                    # ðŸ•µï¸ æ™ºèƒ½æ ¼å¼è¯†åˆ«
                    if "proxies:" in text or url.endswith(".yaml") or url.endswith(".yml"):
                        nodes.extend(self._parse_yaml(text))

                    elif self._is_likely_base64(text):
                        decoded = self._safe_base64_decode(text)
                        if decoded:
                            nodes.extend(self._extract_links(decoded))

                    else:
                        nodes.extend(self._extract_links(text))
                        nodes.extend(self._extract_raw_ips(text))

            return nodes, True
        except Exception:
            return [], False

    def _parse_yaml(self, text: str) -> List[Dict[str, Any]]:
        nodes = []
        try:
            data = yaml.safe_load(text)
            proxies = data.get('proxies', [])
            for p in proxies:
                node = self._convert_clash_proxy(p)
                if node: nodes.append(node)
        except:
            pass
        return nodes

    def _convert_clash_proxy(self, p: Dict) -> Dict:
        try:
            proto = p.get('type')
            # ðŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå…è®¸ socks5 å’Œ http
            if proto not in ['vmess', 'ss', 'trojan', 'vless', 'socks5', 'http']: return None

            return {
                "id": f"clash_{proto}_{p.get('server')}_{p.get('port')}",
                "name": p.get('name'),
                "protocol": proto,
                "host": p.get('server'),
                "port": int(p.get('port')),
                "uuid": p.get('uuid'),
                "password": p.get('password'),
                "cipher": p.get('cipher'),
                "network": p.get('network', 'tcp'),
                "tls": 'tls' if p.get('tls') else 'none',
                "sni": p.get('servername'),
                "path": p.get('ws-path') or p.get('ws-opts', {}).get('path'),
                "host_header": p.get('ws-headers', {}).get('Host')
            }
        except:
            return None

    def _extract_links(self, text: str) -> List[Dict[str, Any]]:
        nodes = []
        for line in text.splitlines():
            line = line.strip()
            if not line: continue
            
            # ðŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ‰‹åŠ¨è§£æž socks5:// å’Œ http:// é“¾æŽ¥
            if line.startswith("socks5://"):
                try:
                    # ç®€å•è§£æž socks5://user:pass@host:port æˆ– socks5://host:port
                    parts = line.replace("socks5://", "").split("@")
                    if len(parts) == 2:
                        auth, server = parts
                        host, port = server.split(":")
                        nodes.append({
                            "id": f"socks5_{host}_{port}",
                            "name": f"SOCKS5 {host}",
                            "protocol": "socks5",
                            "host": host,
                            "port": int(port),
                        })
                    else:
                        host, port = parts[0].split(":")
                        nodes.append({
                            "id": f"socks5_{host}_{port}",
                            "name": f"SOCKS5 {host}",
                            "protocol": "socks5",
                            "host": host,
                            "port": int(port),
                        })
                except: pass
                continue

            node = parse_node_url(line)
            if node: nodes.append(node)
        return nodes

    def _extract_raw_ips(self, text: str) -> List[Dict[str, Any]]:
        nodes = []
        regex = r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d+)'
        matches = re.findall(regex, text)
        for ip, port in matches:
            nodes.append({
                "id": f"cn_http_{ip}_{port}",
                "name": f"HTTP Proxy {ip}",
                "protocol": "http",
                "host": ip,
                "port": int(port),
            })
        return nodes

    def _is_likely_base64(self, text: str) -> bool:
        clean = text.strip()
        if " " in clean or "\n" in clean or len(clean) < 20: return False
        if clean.startswith("vmess://") or clean.startswith("ss://"): return False
        return True

    def _safe_base64_decode(self, text: str) -> str:
        try:
            text = text.strip()
            return base64.b64decode(text + '=' * (-len(text) % 4)).decode('utf-8', errors='ignore')
        except:
            return None
