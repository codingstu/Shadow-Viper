# backend/app/modules/crawler/crawler_engine.py
import asyncio
import json
import os
import time
import pandas as pd
from fastapi import APIRouter, Response
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel

# å¼•å…¥ç‹¬ç«‹çš„çˆ¬è™«å®ç°å’Œä»£ç†æµ
from .crawlers.text_crawler import GeneralTextCrawler
from .crawlers.video_crawler import BilibiliCrawler, YouTubeCrawler, UniversalVideoCrawler
from .proxy import router as proxy_router

router = APIRouter(tags=["crawler"])

# å°†ä»£ç†æµè·¯ç”±åŒ…å«è¿›æ¥
router.include_router(proxy_router)

class CrawlRequest(BaseModel):
    url: str
    mode: str = "auto"
    network_type: str = "proxy" # ğŸ”¥ æ–°å¢ï¼šproxy(è‡ªåŠ¨/ä»£ç†æ± ), node(ä»…èŠ‚ç‚¹), direct(ä»…ç›´è¿)

VIDEO_SITES = [
    "missav", "missav.ws", "jable", "pornhub", "xvideos",
    "youtube", "youtu.be", "bilibili", "spankbang", "ddh", "jav", "hqporner"
]

def is_video_site(url: str) -> bool:
    return any(site in url.lower() for site in VIDEO_SITES)

class CrawlerFactory:
    @staticmethod
    def get_crawler(url: str, mode: str):
        if "bilibili.com" in url:
            return BilibiliCrawler()
        elif "youtube.com" in url or "youtu.be" in url:
            return YouTubeCrawler()
        elif is_video_site(url) or mode == "media":
            return UniversalVideoCrawler()
        return GeneralTextCrawler()

async def smart_router(url: str, mode: str, network_type: str):
    yield json.dumps({"step": "init", "message": f"ä»»åŠ¡å¯åŠ¨: {url} [Mode: {mode}] [Net: {network_type}]"}) + "\n"
    await asyncio.sleep(0.5)

    crawler = CrawlerFactory.get_crawler(url, mode)

    try:
        df = pd.DataFrame()
        # ğŸ”¥ å°† network_type ä¼ é€’ç»™ crawl æ–¹æ³•
        async for chunk in crawler.crawl(url, network_type=network_type):
            if isinstance(chunk, pd.DataFrame):
                df = pd.concat([df, chunk], ignore_index=True)
            else:
                yield chunk

        if not df.empty:
            json_data = df.to_json(orient='records', force_ascii=False)
            yield json.dumps({
                "step": "done", 
                "data": json.loads(json_data),
                "columns": df.columns.tolist()
            }) + "\n"
        else:
            yield json.dumps({"step": "error", "message": "æœªèƒ½æå–åˆ°æœ‰æ•ˆæ•°æ®"}) + "\n"

    except Exception as e:
        yield json.dumps({"step": "error", "message": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"}) + "\n"

@router.post("/api/crawl")
async def start_crawl(request: CrawlRequest):
    # ğŸ”¥ ä¼ é€’ network_type
    return StreamingResponse(smart_router(request.url, request.mode, request.network_type), media_type="application/x-ndjson")

@router.get("/download/{filename}")
async def download_file(filename: str):
    filepath = os.path.abspath(filename)
    if os.path.exists(filepath):
        return FileResponse(filepath, filename=filename)
    return Response("File not found", status_code=404)
