# backend/app/modules/crawler/crawlers/text_crawler.py
import asyncio
import json
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Route
from urllib.parse import urlparse
from abc import ABC, abstractmethod
import re
from typing import List, Dict

# å¼•å…¥ä»£ç†æ± ç®¡ç†å™¨ (ç›¸å¯¹å¯¼å…¥)
try:
    from ...proxy.proxy_engine import manager as pool_manager
except ImportError:
    pool_manager = None

GLOBAL_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"


class BaseCrawler(ABC):
    def __init__(self, pool_manager=None):
        self.pool_manager = pool_manager

    @abstractmethod
    async def crawl(self, url: str, network_type: str = "proxy", force_browser: bool = False):
        pass

    async def get_playwright_proxy(self, network_type="auto"):
        if network_type == "direct":
            return None, "Direct"

        proxy_config = None
        proxy_name = "Direct (Fallback)"

        if self.pool_manager and network_type in ["proxy", "auto", "node"]:
            # æ¯æ¬¡è·å–éƒ½éšæœºå–ä¸€ä¸ªé«˜è´¨é‡èŠ‚ç‚¹ï¼Œç¡®ä¿é‡è¯•æ—¶èƒ½æ¢ IP
            alive_nodes = [p for p in self.pool_manager.proxies if p.score > 0]
            if alive_nodes:
                # éšæœºæ€§æ›´å¤§ä¸€ç‚¹ï¼Œé˜²æ­¢ä¸€ç›´éšåˆ°åŒä¸€ä¸ª
                p = random.choice(alive_nodes[:20] if len(alive_nodes) > 20 else alive_nodes)
                proxy_config = {"server": p.to_url()}
                proxy_name = f"ğŸŒ Proxy-{p.ip}"
                return proxy_config, proxy_name

        return None, "Direct (Fallback)"


async def async_request(method, url, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: requests.request(method, url, **kwargs))


async def request_with_chain_async(url, headers=None, stream=False, timeout=15, method="GET", network_type="proxy",
                                   pool_manager=None):
    if headers is None: headers = {}
    domain = urlparse(url).netloc
    referer = "https://juejin.cn/" if "juejin" in domain else "https://www.google.com/"

    base_headers = {
        "User-Agent": GLOBAL_USER_AGENT, "Accept-Language": "zh-CN,zh;q=0.9",
        "Referer": referer, "Upgrade-Insecure-Requests": "1"
    }
    base_headers.update(headers)

    chain = []
    if network_type == "direct":
        chain.append((None, "Direct", 10))
    else:
        if pool_manager:
            alive_nodes = [p for p in pool_manager.proxies if p.score > 0]
            if alive_nodes:
                selected = sorted(alive_nodes, key=lambda p: p.score, reverse=True)[:5]
                for p in selected:
                    chain.append((p.to_url(), f"ğŸŒ Proxy-{p.ip}", 10))
        chain.append((None, "Direct (Fallback)", 10))

    if not chain:
        chain.append((None, "Direct (Emergency)", 10))

    last_error = None
    for proxy_url, name, time_limit in chain:
        try:
            proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
            resp = await async_request(
                method, url, headers=base_headers, proxies=proxies,
                timeout=time_limit, verify=False, stream=stream
            )
            if resp.status_code in [200, 304]:
                resp.network_name = name
                return resp
            last_error = f"{name} Error ({resp.status_code})"
        except Exception as e:
            last_error = f"{name} Exception: {str(e)}"

    class DummyResponse:
        status_code = 500;
        text = "";
        content = b"";
        network_name = f"Failed. Last: {last_error}"

        def json(self): return {}

    return DummyResponse()


async def block_aggressive(route):
    if route.request.resource_type in ["image", "font", "media"]:
        await route.abort()
    else:
        await route.continue_()


class GeneralTextCrawler(BaseCrawler):
    def extract_text_from_html(self, html: str) -> List[Dict]:
        soup = BeautifulSoup(html, "html.parser")
        data_list = []

        if soup.title: data_list.append({"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": soup.title.string.strip(), "å¤‡æ³¨": "Meta-Title"})

        art = soup.select_one(".article-content") or soup.select_one(".markdown-body")
        if art: data_list.append({"ç±»å‹": "ARTICLE", "å†…å®¹": art.get_text("\n", strip=True), "å¤‡æ³¨": "Juejin Article"})

        # å¢å¼ºçš„é€‰æ‹©å™¨ (é’ˆå¯¹æ˜é‡‘å¤šå˜çš„ç»“æ„)
        selectors = [
            ".comment-list-wrapper .comment-item",
            "div[class*='comment-item']",
            ".comment-list .item",
            "div[data-test-id='comment-item']",
            ".comments-container .comment"
        ]

        juejin_comments = []
        for sel in selectors:
            found = soup.select(sel)
            if found:
                juejin_comments = found
                break

        for item in juejin_comments:
            content = item.select_one(".comment-content") or item.select_one("div[class*='content']")
            if content:
                user = item.select_one(".user-name") or item.select_one(".name")
                u_text = user.get_text(strip=True) if user else "User"
                data_list.append({"ç±»å‹": "è¯„è®º", "å†…å®¹": content.get_text(strip=True), "å¤‡æ³¨": f"User: {u_text}"})

        if data_list: return data_list

        gen_art = soup.find("article") or soup.find("div", class_=re.compile(r'post|article|content', re.I))
        if gen_art:
            t = gen_art.get_text("\n", strip=True)
            if len(t) > 20: data_list.append({"ç±»å‹": "ARTICLE", "å†…å®¹": t, "å¤‡æ³¨": "General Article"})

        for c in soup.select("div[class*='comment'], div[class*='reply']")[:50]:
            t = c.get_text(strip=True)
            if 10 < len(t) < 500: data_list.append({"ç±»å‹": "è¯„è®º", "å†…å®¹": t, "å¤‡æ³¨": "General Comment"})

        return data_list

    async def extract_text_async(self, html):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self.extract_text_from_html, html)

    async def crawl(self, url: str, network_type: str = "proxy", force_browser: bool = False):
        # 1. é™æ€æŠ“å– (Request Phase)
        if not force_browser:
            yield json.dumps(
                {"step": "process", "message": f"ğŸš€ å¯åŠ¨æé€Ÿæ–‡æœ¬è§£æ (Requests) [{network_type}]..."}) + "\n"
            resp = await request_with_chain_async(url, network_type=network_type, pool_manager=self.pool_manager)

            if resp.status_code == 599:
                yield json.dumps({"step": "error", "message": f"âŒ {resp.network_name}"}) + "\n"
                return

            net_name = getattr(resp, "network_name", "æœªçŸ¥")
            data_list = []
            if resp.status_code in [200, 304] and len(resp.text) > 100:
                resp.encoding = 'utf-8'
                data_list = await self.extract_text_async(resp.text)

            is_juejin = "juejin.cn" in url
            has_comments = any(d['ç±»å‹'] == 'è¯„è®º' for d in data_list)

            if data_list and (not is_juejin or has_comments):
                yield json.dumps({"step": "process",
                                  "message": f"ğŸŒ é™æ€æå–æˆåŠŸ ({net_name}) - å‘ç° {len(data_list)} æ¡æ•°æ®"}) + "\n"
                yield pd.DataFrame(data_list)
                return

            yield json.dumps({"step": "process", "message": f"âš ï¸ é™æ€æŠ“å–ä¸æ»¡è¶³ ({net_name})ï¼Œå¯åŠ¨æµè§ˆå™¨æ¸²æŸ“..."}) + "\n"
        else:
            yield json.dumps({"step": "process", "message": f"ğŸ–¥ï¸ ç”¨æˆ·å¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¸²æŸ“..."}) + "\n"

        # ==================== ğŸ”¥ æ ¸å¿ƒå¢å¼ºï¼šPlaywright è‡ªåŠ¨é‡è¯•æœºåˆ¶ (3æ¡å‘½) ğŸ”¥ ====================
        max_retries = 3
        # å¦‚æœæ˜¯ç›´è¿ï¼Œåªè¯•ä¸€æ¬¡ï¼Œå› ä¸ºç½‘ç»œç¯å¢ƒä¸å˜é‡è¯•æ²¡æ„ä¹‰
        if network_type == "direct": max_retries = 1

        for attempt in range(1, max_retries + 1):
            try:
                # æ¯æ¬¡å°è¯•éƒ½é‡æ–°è·å–ä¸€ä¸ªæ–°ä»£ç†
                proxy_conf, proxy_name = await self.get_playwright_proxy(network_type)
                yield json.dumps({"step": "process",
                                  "message": f"ğŸŒ [ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯•] å¯åŠ¨æµè§ˆå™¨: {proxy_name}..."}) + "\n"

                async with async_playwright() as p:
                    browser = None
                    try:
                        # å¢åŠ é˜²æ£€æµ‹å‚æ•°
                        browser = await p.chromium.launch(
                            headless=True,
                            args=["--mute-audio", "--disable-blink-features=AutomationControlled"],
                            proxy=proxy_conf
                        )
                        context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                        page = await context.new_page()
                        await page.route("**/*", block_aggressive)

                        # å¢åŠ è¶…æ—¶å®¹é”™
                        await page.goto(url, timeout=45000, wait_until="networkidle")

                        # æ˜é‡‘ä¸“ç”¨é€»è¾‘
                        if "juejin.cn" in url:
                            yield json.dumps({"step": "process", "message": "ğŸ–±ï¸ æ£€æµ‹åˆ°æ˜é‡‘ï¼Œæ­£åœ¨è´ªå©ªæŠ“å–..."}) + "\n"

                            try:
                                await page.click(".fetch-comment-btn", timeout=3000)
                                await asyncio.sleep(2)
                            except:
                                pass

                            # å¢åŠ æ»šåŠ¨ç­‰å¾…æ—¶é—´ï¼Œåº”å¯¹æ…¢èŠ‚ç‚¹
                            prev_height = 0
                            for i in range(10):  # æ»š10æ¬¡
                                await page.keyboard.press("End")
                                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                # æ…¢ç½‘é€Ÿä¸‹ï¼Œç»™ 3 ç§’åŠ è½½æ—¶é—´
                                await asyncio.sleep(3)

                                new_height = await page.evaluate("document.body.scrollHeight")
                                if new_height == prev_height:
                                    # å°è¯•æ™ƒåŠ¨
                                    await page.evaluate("window.scrollBy(0, -300)")
                                    await asyncio.sleep(1)
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                    await asyncio.sleep(2)
                                    if await page.evaluate("document.body.scrollHeight") == new_height:
                                        break
                                prev_height = new_height
                                if i % 2 == 0:
                                    yield json.dumps(
                                        {"step": "process", "message": f"ğŸ–±ï¸ æ»šåŠ¨åŠ è½½ä¸­ ({i + 1})..."}) + "\n"

                        else:
                            # é€šç”¨æ»šåŠ¨
                            for _ in range(5):
                                await page.evaluate("window.scrollBy(0, window.innerHeight)")
                                await asyncio.sleep(1)

                        content = await page.content()
                        data_list = await self.extract_text_async(content)

                        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                        if data_list:
                            c_count = len([d for d in data_list if d['ç±»å‹'] == 'è¯„è®º'])
                            yield json.dumps({"step": "process",
                                              "message": f"âœ… æ·±åº¦æ¸²æŸ“æå–æˆåŠŸ - å‘ç° {len(data_list)} æ¡æ•°æ® ({c_count} æ¡è¯„è®º)"}) + "\n"
                            yield pd.DataFrame(data_list)
                            await browser.close()
                            return  # ğŸ”¥ æˆåŠŸå°±é€€å‡ºå‡½æ•°ï¼Œä¸å†é‡è¯•

                        # å¦‚æœæ²¡æŠ“åˆ°æ•°æ®ï¼ŒæŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
                        raise Exception("é¡µé¢åŠ è½½æˆåŠŸä½†æœªæå–åˆ°æœ‰æ•ˆæ•°æ® (å¯èƒ½æ˜¯ç™½å±æˆ–è¢«æ‹¦æˆª)")

                    except Exception as e:
                        err_msg = str(e)
                        # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œæ‰æŠ¥ Error
                        if attempt == max_retries:
                            yield json.dumps({"step": "error", "message": f"âŒ æœ€ç»ˆå¤±è´¥: {err_msg}"}) + "\n"
                        else:
                            # å¦åˆ™åªæŠ¥ Warningï¼Œå¹¶ç»§ç»­å¾ªç¯
                            yield json.dumps({"step": "process",
                                              "message": f"âš ï¸ å½“å‰èŠ‚ç‚¹ ({proxy_name}) ä¸ç¨³å®š: {err_msg}ï¼Œå‡†å¤‡åˆ‡æ¢èŠ‚ç‚¹é‡è¯•..."}) + "\n"
                    finally:
                        if browser: await browser.close()

            except Exception as outer_e:
                # æ•è·è·å–ä»£ç†ç­‰å¤–éƒ¨é”™è¯¯
                if attempt == max_retries:
                    yield json.dumps({"step": "error", "message": f"âŒ å¯åŠ¨å¤±è´¥: {str(outer_e)}"}) + "\n"