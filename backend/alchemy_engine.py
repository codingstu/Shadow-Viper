# backend/alchemy_engine.py
import json
import asyncio
import random
import re
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

try:
    from ai_hub import call_ai
except ImportError:
    call_ai = None

router = APIRouter(prefix="/api/alchemy", tags=["alchemy"])


class DeAIRequest(BaseModel):
    text: str


WRITER_MODEL = "deepseek-ai/DeepSeek-V3"
JUDGE_MODEL = "deepseek-ai/DeepSeek-R1"


# ==================== 1. å·¥å…·å‡½æ•°ï¼šæå–æ€ç»´é“¾ ====================
def extract_think_content(text):
    """åˆ†ç¦» <think> å†…å®¹å’Œæ­£æ–‡"""
    if not text: return None, None
    think_content = None
    clean_text = text

    # æå– <think>...</think>
    match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)
    if match:
        think_content = match.group(1).strip()
        clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()

    # æ¸…æ´— Markdown JSON åŒ…è£¹
    clean_text = clean_text.replace("```json", "").replace("```", "").strip()
    return think_content, clean_text

# ==================== 2. è£åˆ¤ç³»ç»Ÿ (å¸¦æ€ç»´é“¾è¿”å›) ====================
# 2. è£åˆ¤ç³»ç»Ÿï¼šæ¸©åº¦è®¾ä¸º 0 ä»¥ä¿è¯ç»“æœç»å¯¹ä¸€è‡´
async def detect_ai_probability(text: str) -> dict:
    # æ›´åŠ ä¸¥è°¨çš„ Promptï¼Œè¦æ±‚å…ˆæ€è€ƒç‰¹å¾ï¼Œå†æ‰“åˆ†ï¼Œé˜²æ­¢ççŒœ
    prompt = (
        "Role: Professional AI Text Forensic Analyst.\n"
        "Task: Analyze the following text and determine the probability (0-100%) that it was written by an AI.\n"
        "Method: \n"
        "1. First, inside <think> tags, analyze the Sentence Length Variance (Burstiness) and Perplexity.\n"
        "2. Look for AI patterns: robotic transitions ('Moreover', 'In conclusion'), repetitive structure, lack of idioms.\n"
        "3. Finally, output the JSON.\n"
        "Output Format: <think>...analysis...</think>\n"
        "JSON ONLY: {\"score\": <int 0-100>, \"reason\": \"<short summary>\"}\n"
        "Constraint: Be consistent. If text is casual and irregular, score low (<10). If text is rigid and textbook-like, score high (>80)."
    )

    try:
        # ğŸ”¥ å…³é”®ï¼štemperature=0 ç¡®ä¿æ¯æ¬¡æ£€æµ‹ç»“æœä¸€è‡´ï¼Œä¸ä¼šå‡ºç°ä¸€æ¬¡12ä¸€æ¬¡95çš„æƒ…å†µ
        raw_text, model_name = call_ai(
            prompt,
            f"TEXT TO ANALYZE:\n{text[:1500]}",
            model=JUDGE_MODEL,
            temperature=0,
            return_model_name=True
        )

        # æå–æ€è€ƒå’Œç»“æœ
        think, json_text = extract_think_content(raw_text)
        data = parse_json_safely(json_text)

        if data:
            return {
                "score": int(data.get("score", 50)),
                "detector": model_name,
                "thinking": think  # å°†æ€è€ƒè¿‡ç¨‹è¿”å›ç»™å‰ç«¯
            }

    except Exception as e:
        return {"score": -1, "detector": f"Error: {str(e)}", "thinking": None}

    return {"score": -1, "detector": "Failed", "thinking": None}

# ==================== 3. æ ¸å¿ƒæµç¨‹ (å®Œå…¨é€æ˜åŒ–) ====================
# backend/alchemy_engine.py (æ–°å¢å·¥å…·å‡½æ•° + æ›¿æ¢ä¸»æµç¨‹)

def parse_json_safely(text):
    try:
        # å°è¯•å¯»æ‰¾æœ€å¤–å±‚çš„ {}
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            return json.loads(text[start:end + 1])
        return json.loads(text)
    except:
        return None

# 2. æ›¿æ¢ï¼šå¸¦é€æ˜åŒ–å±•ç¤ºçš„ä¸»æµç¨‹
# 3. æ ¸å¿ƒæµç¨‹ï¼šå®æ—¶æ¨é€æ€ç»´é“¾
async def chaos_pipeline(source_text: str):
    try:
        yield json.dumps({"step": "init", "msg": "ğŸ”Œ å¯åŠ¨ DeepSeek é€æ˜åŒ–å¼•æ“ (CoT Visible)..."}) + "\n"
        await asyncio.sleep(0.5)

        # --- Phase 1: åˆå§‹æ£€æµ‹ ---
        yield json.dumps({"step": "thought", "msg": "ğŸ•µï¸â€â™‚ï¸ è£åˆ¤ (DeepSeek R1) æ­£åœ¨æ·±åº¦å®¡è§† (Temp=0)..."}) + "\n"
        check = await detect_ai_probability(source_text)

        # ğŸ”¥ å®æ—¶å±•ç¤ºè£åˆ¤çš„æ€è€ƒè¿‡ç¨‹
        if check.get("thinking"):
            yield json.dumps(
                {"step": "process", "msg": f"ğŸ§  [è£åˆ¤æ€è€ƒ]:\n{check['thinking'][:300]}...\n(åˆ†æå®Œæ¯•)"}) + "\n"

        current_score = check["score"]
        if current_score == -1:
            yield json.dumps({"step": "error", "msg": "âŒ æ£€æµ‹å¤±è´¥: è¯·æ£€æŸ¥ SILICON_API_KEY"}) + "\n"
            return

        yield json.dumps(
            {"step": "detected", "score": current_score, "msg": f"åˆå§‹: {current_score}% ({check['detector']})"}) + "\n"

        # --- Phase 2: æ™ºèƒ½è·³è¿‡ ---
        target_score = 10
        if current_score <= 15:
            yield json.dumps({"step": "process", "msg": "âœ… åˆ†æ•°å·²è¾¾æ ‡ï¼Œæ­£åœ¨è¿›è¡Œå¾®è°ƒæ¶¦è‰²..."}) + "\n"

            prompt = "Polish this text to make it flow naturally like a native speaker. Do not change the meaning."
            # è°ƒç”¨ V3 å¾®è°ƒ
            raw_res, model_name = call_ai(prompt, source_text, model=WRITER_MODEL, return_model_name=True)
            think, final_text = extract_think_content(raw_res)

            if think:
                yield json.dumps({"step": "process", "msg": f"ğŸ§  [æ¶¦è‰²æ€è€ƒ]:\n{think[:150]}..."}) + "\n"

            yield json.dumps({"step": "done", "result": final_text, "final_score": current_score,
                              "msg": f"å·²è¾¾æ ‡ | å¾®è°ƒæ¨¡å‹: {model_name}"}) + "\n"
            return

        # --- Phase 3: æ·±åº¦é™é‡ ---
        current_text = source_text
        best_text = source_text
        best_score = current_score

        # ç¡®ä¿ strategies æ ¼å¼æ­£ç¡® (2å…ƒç´ å…ƒç»„)
        strategies = [
            ("æ·±åº¦æ‹Ÿäºº", "Rewrite to sound like a human expert. Use variable sentence lengths. **NO LISTS**."),
            ("ç»“æ„æ‰“æ•£", "Completely change sentence structure. Combine short sentences. **NO LISTS**."),
            ("æš´åŠ›å£è¯­", "Explain this casually. Use idioms. **NO FORMATTING**.")
        ]

        MAX_ATTEMPTS = 3
        attempt = 0

        while current_score > target_score and attempt < MAX_ATTEMPTS:
            attempt += 1
            if attempt > len(strategies): break

            strategy_name, prompt_instruction = strategies[attempt - 1]

            yield json.dumps({"step": "thought", "msg": f"ğŸ”„ [Round {attempt}] æ‰§è¡Œç­–ç•¥: {strategy_name}..."}) + "\n"

            # æ‰§è¡Œç”Ÿæˆ
            raw_res, model_name = call_ai(prompt_instruction, current_text, model=WRITER_MODEL, return_model_name=True)
            think, temp_text = extract_think_content(raw_res)

            # ğŸ”¥ å±•ç¤ºå†™æ‰‹çš„æ€è€ƒ
            if think:
                yield json.dumps({"step": "process", "msg": f"ğŸ§  [å†™æ‰‹æ€è€ƒ]:\n{think[:200]}..."}) + "\n"

            yield json.dumps({"step": "update_view", "content": temp_text}) + "\n"

            # å¤æ£€
            yield json.dumps({"step": "thought", "msg": "ğŸ” è£åˆ¤å¤æ£€ä¸­..."}) + "\n"
            new_check = await detect_ai_probability(temp_text)

            if new_check.get("thinking"):
                yield json.dumps({"step": "process", "msg": f"ğŸ§  [å¤æ£€æ€è€ƒ]:\n{new_check['thinking'][:150]}..."}) + "\n"

            new_score = new_check["score"]

            # æ­¢æŸé€»è¾‘
            if new_score > current_score + 10:
                yield json.dumps(
                    {"step": "ai_warn", "msg": f"âš ï¸ è­¦å‘Š: åˆ†æ•°æ¶åŒ– ({current_score}% -> {new_score}%)ï¼Œå›æ»š..."}) + "\n"
                current_text = best_text
            elif new_score <= current_score:
                yield json.dumps({"step": "process", "msg": f"ğŸ“‰ ä¼˜åŒ–æˆåŠŸ: {current_score}% -> {new_score}%"}) + "\n"
                current_text = temp_text
                current_score = new_score
                best_text = temp_text
                best_score = new_score
            else:
                current_text = temp_text
                current_score = new_score

            if current_score <= target_score:
                break

        yield json.dumps({
            "step": "done",
            "result": best_text,
            "final_score": best_score,
            "msg": f"æœ€ç»ˆ: {best_score}% | è£åˆ¤: {new_check['detector']}"
        }) + "\n"

    except Exception as e:
        yield json.dumps({"step": "error", "msg": f"Err: {str(e)}"}) + "\n"


@router.post("/de_ai")
async def de_ai_endpoint(req: DeAIRequest):
    return StreamingResponse(chaos_pipeline(req.text), media_type="application/x-ndjson")