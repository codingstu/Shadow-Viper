# backend/alchemy_engine.py
import json
import asyncio
import re
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# å¼•å…¥æµå¼å’Œæ™®é€šè°ƒç”¨
try:
    from ai_hub import call_ai, call_ai_stream_async
except ImportError:
    call_ai = None
    call_ai_stream_async = None

router = APIRouter(prefix="/api/alchemy", tags=["alchemy"])


class DeAIRequest(BaseModel):
    text: str


# ==================== âš™ï¸ é…ç½®åŒºåŸŸ ====================
# DeepSeek R1: è´Ÿè´£é€»è¾‘æ¨ç†ã€ç‰¹å¾åˆ†æ (æ€ç»´é“¾é•¿ï¼Œé€Ÿåº¦æ…¢ï¼Œä½†ç²¾å‡†)
JUDGE_MODEL = "deepseek-ai/DeepSeek-R1"
# DeepSeek V3: è´Ÿè´£æ–‡æœ¬ç”Ÿæˆã€æ¶¦è‰² (é€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½)
WRITER_MODEL = "deepseek-ai/DeepSeek-V3"


# ==================== ğŸ›  å·¥å…·å‡½æ•° ====================
def extract_think_content(text):
    """åˆ†ç¦» <think> å†…å®¹å’Œæ­£æ–‡"""
    if not text: return None, None
    think_content = None
    clean_text = text

    # æå– <think>...</think>
    match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)
    if match:
        think_content = match.group(1).strip()
        clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()

    # æ¸…æ´— Markdown JSON åŒ…è£¹
    clean_text = clean_text.replace("```json", "").replace("```", "").strip()
    return think_content, clean_text


def parse_json_safely(text):
    try:
        # å°è¯•å¯»æ‰¾æœ€å¤–å±‚çš„ {}
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            return json.loads(text[start:end + 1])
        return json.loads(text)
    except:
        return None


# ==================== ğŸ•µï¸â€â™‚ï¸ è£åˆ¤ç³»ç»Ÿ (æµå¼å¢å¼ºç‰ˆ) ====================
async def stream_judge_logic(text: str):
    """
    æµå¼æ‰§è¡Œè£åˆ¤é€»è¾‘ï¼š
    1. å®æ—¶æ¨é€ R1 çš„æ€è€ƒè¿‡ç¨‹ (è§£å†³å¡é¡¿ç„¦è™‘)
    2. æœ€åè¿”å›å®Œæ•´çš„è¯„åˆ†ç»“æœ
    """
    prompt = (
        "Role: Professional AI Text Forensic Analyst.\n"
        "Task: Analyze the following text and determine the probability (0-100%) that it was written by an AI.\n"
        "Method: \n"
        "1. First, inside <think> tags, analyze the Sentence Length Variance (Burstiness) and Perplexity.\n"
        "2. Look for AI patterns: robotic transitions ('Moreover', 'In conclusion'), repetitive structure, lack of idioms.\n"
        "3. Finally, output the JSON.\n"
        "Output Format: <think>...analysis...</think>\n"
        "JSON ONLY: {\"score\": <int 0-100>, \"reason\": \"<short summary>\"}\n"
        "Constraint: Be consistent. If text is casual and irregular, score low (<10). If text is rigid and textbook-like, score high (>80)."
    )

    full_response = ""
    in_think_block = False
    
    # æ¨¡æ‹Ÿä¸€ä¸ªåˆå§‹çš„æ€è€ƒæ—¥å¿—
    yield {"type": "log", "msg": f"ğŸ”— è¿æ¥æ¨¡å‹: {JUDGE_MODEL}..."}
    
    try:
        # ä½¿ç”¨ ai_hub çš„å¼‚æ­¥æµå¼æ¥å£
        async for chunk in call_ai_stream_async(prompt, f"TEXT TO ANALYZE:\n{text[:1500]}", model=JUDGE_MODEL, temperature=0):
            full_response += chunk
            
            # --- å®æ—¶è§£ææ€ç»´é“¾ ---
            # ç®€å•çš„çŠ¶æ€æœºå¤„ç† <think> æ ‡ç­¾
            if "<think>" in chunk:
                in_think_block = True
                chunk = chunk.replace("<think>", "") # ç§»é™¤æ ‡ç­¾å±•ç¤ºå†…å®¹
            
            if "</think>" in chunk:
                in_think_block = False
                # æˆªå– </think> ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºæœ€åä¸€æ®µæ€è€ƒ
                parts = chunk.split("</think>")
                if parts[0]:
                    yield {"type": "think", "delta": parts[0]}
                continue

            if in_think_block:
                # å®æ—¶æ¨é€æ€è€ƒç‰‡æ®µ
                yield {"type": "think", "delta": chunk}
                
    except Exception as e:
        yield {"type": "error", "msg": str(e)}
        return

    # æµå¼ç»“æŸåï¼Œè§£ææœ€ç»ˆç»“æœ
    think, json_text = extract_think_content(full_response)
    data = parse_json_safely(json_text)

    if data:
        yield {
            "type": "result", 
            "data": {
                "score": int(data.get("score", 50)),
                "detector": JUDGE_MODEL,
                "thinking": think
            }
        }
    else:
        # ğŸ”¥ å¤±è´¥å›é€€æœºåˆ¶ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æš´åŠ›æå–åˆ†æ•°
        score_match = re.search(r'"score":\s*(\d+)', full_response)
        if score_match:
            fallback_score = int(score_match.group(1))
            yield {
                "type": "result", 
                "data": {
                    "score": fallback_score,
                    "detector": f"{JUDGE_MODEL} (Fallback)",
                    "thinking": think
                }
            }
        else:
            yield {"type": "result", "data": {"score": -1, "detector": "Parse Error", "thinking": think}}


# ==================== ğŸŒª æ ¸å¿ƒæµç¨‹ (Chaos Pipeline) ====================
async def chaos_pipeline(source_text: str):
    try:
        yield json.dumps({"step": "init", "msg": "ğŸ”Œ å¯åŠ¨ DeepSeek é€æ˜åŒ–å¼•æ“ (CoT Streaming)..."}) + "\n"
        await asyncio.sleep(0.5)

        # --- Phase 1: åˆå§‹æ£€æµ‹ (æµå¼) ---
        yield json.dumps({"step": "thought", "msg": f"ğŸ•µï¸â€â™‚ï¸ è£åˆ¤ ({JUDGE_MODEL}) æ­£åœ¨ä»‹å…¥..."}) + "\n"
        
        current_score = 50
        check_data = {}
        
        # æ¶ˆè´¹è£åˆ¤çš„æµ
        think_buffer = ""
        async for event in stream_judge_logic(source_text):
            if event["type"] == "log":
                yield json.dumps({"step": "process", "msg": event["msg"]}) + "\n"
            elif event["type"] == "think":
                # ç´¯ç§¯æ€è€ƒå†…å®¹ï¼Œæ¯éš”ä¸€å®šé•¿åº¦æ¨é€ä¸€æ¬¡ï¼Œæˆ–è€…ç›´æ¥æ¨é€å¢é‡
                # ä¸ºäº†å‰ç«¯å±•ç¤ºæµç•…ï¼Œè¿™é‡Œæˆ‘ä»¬æ¨é€å¢é‡ï¼Œå‰ç«¯éœ€è¦æ”¯æŒè¿½åŠ ï¼Œæˆ–è€…æˆ‘ä»¬æ¨é€å®Œæ•´çš„ buffer
                think_buffer += event["delta"]
                # é™åˆ¶æ¨é€é¢‘ç‡æˆ–é•¿åº¦ï¼Œè¿™é‡Œç®€åŒ–ä¸ºæ¯æ”¶åˆ°ä¸€ç‚¹å°±æ¨ä¸€ç‚¹ï¼Œå‰ç«¯å±•ç¤ºä¸ºâ€œæ­£åœ¨æ€è€ƒ...â€
                # æ³¨æ„ï¼šå‰ç«¯å¦‚æœä¸æ”¯æŒæµå¼è¿½åŠ ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦ä¼˜åŒ–ã€‚
                # å‡è®¾å‰ç«¯æ˜¯è¦†ç›–å¼æ˜¾ç¤º msgï¼Œæˆ‘ä»¬æ¨é€æœ€æ–°çš„ buffer å°¾éƒ¨
                yield json.dumps({"step": "process", "msg": f"ğŸ§  [R1 æ·±åº¦æ€è€ƒä¸­]:\n{think_buffer[-300:]}..."}) + "\n"
            elif event["type"] == "result":
                check_data = event["data"]
            elif event["type"] == "error":
                yield json.dumps({"step": "error", "msg": f"API Error: {event['msg']}"}) + "\n"
                return

        current_score = check_data.get("score", -1)
        
        # å±•ç¤ºå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ (å¦‚æœä¹‹å‰åªå±•ç¤ºäº†ç‰‡æ®µ)
        if check_data.get("thinking"):
             yield json.dumps({"step": "process", "msg": f"ğŸ§  [æ€è€ƒå®Œæˆ]:\n{check_data['thinking'][:500]}...\n(é€»è¾‘é—­ç¯)"}) + "\n"

        if current_score == -1:
            yield json.dumps({"step": "error", "msg": "âŒ æ£€æµ‹å¤±è´¥: æ— æ³•è§£æè£åˆ¤ç»“æœ"}) + "\n"
            return

        yield json.dumps(
            {"step": "detected", "score": current_score, "msg": f"åˆå§‹åˆ¤å®š: {current_score}% (ç”± {check_data['detector']} è£å†³)"}) + "\n"

        # --- Phase 2: æ™ºèƒ½è·³è¿‡ ---
        target_score = 10
        if current_score <= 15:
            yield json.dumps({"step": "process", "msg": "âœ… åˆ†æ•°å·²è¾¾æ ‡ï¼Œå¯åŠ¨ V3 å¾®è°ƒæ¨¡å¼..."}) + "\n"
            
            # V3 å¾®è°ƒä¹Ÿå¯ä»¥æµå¼ï¼Œä½†è¿™é‡Œä¸ºäº†ç®€å•ä¿æŒæ™®é€šè°ƒç”¨ï¼Œå› ä¸ºå®ƒå¾ˆå¿«
            prompt = "Polish this text to make it flow naturally like a native speaker. Do not change the meaning."
            raw_res, model_name = call_ai(prompt, source_text, model=WRITER_MODEL, return_model_name=True)
            _, final_text = extract_think_content(raw_res)

            yield json.dumps({"step": "done", "result": final_text, "final_score": current_score,
                              "msg": f"å·²è¾¾æ ‡ | æ¶¦è‰²æ¨¡å‹: {model_name}"}) + "\n"
            return

        # --- Phase 3: æ·±åº¦é™é‡ ---
        current_text = source_text
        best_text = source_text
        best_score = current_score

        strategies = [
            ("æ·±åº¦æ‹Ÿäºº", "Rewrite to sound like a human expert. Use variable sentence lengths. **NO LISTS**."),
            ("ç»“æ„æ‰“æ•£", "Completely change sentence structure. Combine short sentences. **NO LISTS**."),
            ("æš´åŠ›å£è¯­", "Explain this casually. Use idioms. **NO FORMATTING**.")
        ]

        MAX_ATTEMPTS = 3
        attempt = 0

        while current_score > target_score and attempt < MAX_ATTEMPTS:
            attempt += 1
            if attempt > len(strategies): break

            strategy_name, prompt_instruction = strategies[attempt - 1]

            yield json.dumps({"step": "thought", "msg": f"ğŸ”„ [Round {attempt}] æ‰§è¡Œç­–ç•¥: {strategy_name} (Model: {WRITER_MODEL})..."}) + "\n"

            # æ‰§è¡Œç”Ÿæˆ (ä½¿ç”¨ V3)
            # è¿™é‡Œä½¿ç”¨æ™®é€šè°ƒç”¨ï¼Œå› ä¸º V3 é€Ÿåº¦å¿«ï¼Œä¸”é€šå¸¸ä¸è¾“å‡º <think>
            raw_res, model_name = call_ai(prompt_instruction, current_text, model=WRITER_MODEL, return_model_name=True)
            think, temp_text = extract_think_content(raw_res)

            if think:
                yield json.dumps({"step": "process", "msg": f"ğŸ§  [å†™æ‰‹æ€è€ƒ]:\n{think[:200]}..."}) + "\n"

            yield json.dumps({"step": "update_view", "content": temp_text}) + "\n"

            # --- å¤æ£€ (åŒæ ·ä½¿ç”¨æµå¼ R1) ---
            yield json.dumps({"step": "thought", "msg": f"ğŸ” è£åˆ¤ ({JUDGE_MODEL}) å¤æ£€ä¸­..."}) + "\n"
            
            new_check_data = {}
            think_buffer = ""
            async for event in stream_judge_logic(temp_text):
                if event["type"] == "think":
                    think_buffer += event["delta"]
                    yield json.dumps({"step": "process", "msg": f"ğŸ§  [å¤æ£€æ€è€ƒ]:\n{think_buffer[-300:]}..."}) + "\n"
                elif event["type"] == "result":
                    new_check_data = event["data"]

            new_score = new_check_data.get("score", 100) # é»˜è®¤é«˜åˆ†é˜²æ­¢è¯¯åˆ¤

            # æ­¢æŸé€»è¾‘
            if new_score > current_score + 10:
                yield json.dumps(
                    {"step": "ai_warn", "msg": f"âš ï¸ è­¦å‘Š: åˆ†æ•°æ¶åŒ– ({current_score}% -> {new_score}%)ï¼Œå›æ»š..."}) + "\n"
                current_text = best_text
            elif new_score <= current_score:
                yield json.dumps({"step": "process", "msg": f"ğŸ“‰ ä¼˜åŒ–æˆåŠŸ: {current_score}% -> {new_score}%"}) + "\n"
                current_text = temp_text
                current_score = new_score
                best_text = temp_text
                best_score = new_score
            else:
                current_text = temp_text
                current_score = new_score

            if current_score <= target_score:
                break

        yield json.dumps({
            "step": "done",
            "result": best_text,
            "final_score": best_score,
            "msg": f"æœ€ç»ˆ: {best_score}% | è£åˆ¤: {JUDGE_MODEL}"
        }) + "\n"

    except Exception as e:
        yield json.dumps({"step": "error", "msg": f"Err: {str(e)}"}) + "\n"


@router.post("/de_ai")
async def de_ai_endpoint(req: DeAIRequest):
    return StreamingResponse(chaos_pipeline(req.text), media_type="application/x-ndjson")
