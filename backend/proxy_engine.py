import asyncio
import aiohttp
import re
import time
import json
import os
import random
from fastapi import APIRouter, BackgroundTasks
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime
from pydantic import BaseModel
from dotenv import load_dotenv

load_dotenv()

# ==================== é…ç½®åŒºåŸŸ ====================
PROXY_STORE_FILE = "valid_proxies.json"

# å¤‡ç”¨ä»£ç† (ç”¨äºè¾…åŠ©æŠ“å–)
UPSTREAM_PROXY = os.getenv("UPSTREAM_PROXY")

# ğŸ”¥ã€ç»å¯¹æ ¸å¿ƒã€‘å¿…é¡»æ˜¯ HTTPSï¼Œå¦åˆ™æ— æ³•ç”¨äº MissAV
TEST_URL = "https://www.google.com"
TIMEOUT = 8

router = APIRouter(prefix="/api/proxy_pool", tags=["proxy_pool"])


class ProxyRecord(BaseModel):
    ip: str
    port: str
    protocol: str
    speed: float
    last_check: str
    source: str
    score: int = 100


class ProxyManager:
    def __init__(self):
        self.proxies = []
        self.is_running = False
        self.logs = []
        self.scheduler = AsyncIOScheduler()
        self.load_from_file()

    def log(self, msg):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {msg}"
        print(log_entry)
        self.logs.insert(0, log_entry)
        if len(self.logs) > 100: self.logs.pop()

    def load_from_file(self):
        if os.path.exists(PROXY_STORE_FILE):
            try:
                with open(PROXY_STORE_FILE, "r") as f:
                    data = json.load(f)
                    self.proxies = [ProxyRecord(**item) for item in data]
                self.log(f"ğŸ“¥ å·²åŠ è½½ {len(self.proxies)} ä¸ªå†å²ä»£ç†")
            except:
                pass

    def save_to_file(self):
        try:
            self.proxies.sort(key=lambda x: x.speed)
            with open(PROXY_STORE_FILE, "w") as f:
                json.dump([p.dict() for p in self.proxies], f)
        except:
            pass

    # --- 1. æŠ“å–æ¨¡å— ---
    async def fetch_public_sources(self):
        candidates = []
        sources = [
            "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/prxchk/proxy-list/main/http.txt",
            "https://raw.githubusercontent.com/zloi-user/hideip.me/main/http.txt",
            "https://www.proxy-list.download/api/v1/get?type=http",
            "https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all"
        ]

        self.log(f"ğŸŒ å¼€å§‹æŠ“å– {len(sources)} ä¸ªæº...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

        async with aiohttp.ClientSession(headers=headers) as session:
            for url in sources:
                content = None
                try:
                    async with session.get(url, timeout=10, ssl=False) as resp:
                        if resp.status == 200: content = await resp.text()
                except:
                    # å¤‡ç”¨ï¼šèµ°ä»˜è´¹ä»£ç†æŠ“å–
                    try:
                        async with session.get(url, proxy=UPSTREAM_PROXY, timeout=15, ssl=False) as resp:
                            if resp.status == 200: content = await resp.text()
                    except:
                        pass

                if content:
                    found = re.findall(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})[:\s](\d+)', content)
                    if found:
                        self.log(f"      â””â”€ æå–åˆ° {len(found)} ä¸ª IP")
                        for ip, port in found:
                            candidates.append({
                                "ip": ip, "port": port, "protocol": "http",
                                "source": "Public", "score": 50
                            })
        return candidates

    # --- 2. éªŒè¯æ¨¡å— ---
    async def validate_one(self, proxy_dict, session):
        proxy_url = f"http://{proxy_dict['ip']}:{proxy_dict['port']}"
        start = time.time()
        try:
            # ğŸ”¥ å¼ºåˆ¶ HTTPS æ¡æ‰‹
            async with session.get(TEST_URL, proxy=proxy_url, timeout=TIMEOUT, ssl=False) as resp:
                if resp.status == 200:
                    speed = int((time.time() - start) * 1000)
                    return {
                        **proxy_dict,
                        "speed": speed,
                        "last_check": datetime.now().strftime("%H:%M:%S"),
                        "protocol": "HTTPS",  # ğŸ”¥ã€æ–°å¢è¿™ä¸€è¡Œã€‘éªŒè¯é€šè¿‡åï¼Œå¼ºåˆ¶æ”¹åä¸º HTTPS
                        "score": 100
                    }
        except:
            return None

    async def run_cycle(self):
        if self.is_running: return
        self.is_running = True
        self.log("ğŸš€ ==== å¼€å§‹ IP ç‹©çŒ (ä¸¥æ ¼HTTPSæ¨¡å¼) ====")

        # 1. æŠ“å–
        candidates = await self.fetch_public_sources()

        # 2. å¤æ´»èµ›ï¼šåªç»™ç°å­˜ä»£ç†ä¸€æ¬¡æœºä¼šï¼Œå¦‚æœè¿™æ¬¡ä¸è¡Œç›´æ¥å‰”é™¤
        for p in self.proxies:
            candidates.append(p.dict())

        # æ¸…ç©ºå½“å‰åˆ—è¡¨ï¼Œé‡æ–°æ´—ç‰Œ
        self.proxies = []
        self.save_to_file()

        # 3. å»é‡
        unique_map = {f"{p['ip']}:{p['port']}": p for p in candidates}
        unique_candidates = list(unique_map.values())
        total = len(unique_candidates)
        self.log(f"âš¡ å¾…éªŒè¯: {total} ä¸ª (æ­£åœ¨æ¸…æ´—éHTTPSä»£ç†...)")

        if total == 0:
            self.is_running = False
            return

        # 4. éªŒè¯
        batch_size = 200
        async with aiohttp.ClientSession() as session:
            for i in range(0, total, batch_size):
                if not self.is_running: break
                batch = unique_candidates[i:i + batch_size]
                tasks = [self.validate_one(p, session) for p in batch]
                results = await asyncio.gather(*tasks)

                new_valid = [r for r in results if r]
                if len(new_valid) > 0:
                    self.proxies.extend([ProxyRecord(**p) for p in new_valid])
                    self.save_to_file()
                    self.log(f"   âœ¨ æ•è· {len(new_valid)} ä¸ª HTTPS ä»£ç†")
                await asyncio.sleep(0.2)

        self.is_running = False
        self.log(f"âœ… æ‰«æç»“æŸã€‚æœ€ç»ˆæœ‰æ•ˆ: {len(self.proxies)} ä¸ª")

    # ğŸ”¥ æ–°å¢ï¼šæ¸…ç©ºåŠŸèƒ½
    def clear_all(self):
        self.proxies = []
        self.save_to_file()
        self.log("ğŸ—‘ï¸ IP æ± å·²æ¸…ç©º")


manager = ProxyManager()


@router.get("/stats")
async def get_stats():
    return {"count": len(manager.proxies), "running": manager.is_running, "logs": manager.logs}


@router.get("/list")
async def get_list():
    return manager.proxies[:100]


@router.get("/pop")
async def get_random_proxy():
    if not manager.proxies: return {"proxy": None}
    choice = random.choice(manager.proxies[:20])
    return {"proxy": f"http://{choice.ip}:{choice.port}"}


@router.post("/trigger")
async def trigger_task(background_tasks: BackgroundTasks):
    if manager.is_running: return {"message": "Busy"}
    background_tasks.add_task(manager.run_cycle)
    return {"message": "Started"}


# ğŸ”¥ æ–°å¢æ¥å£
@router.delete("/clean")
async def clean_pool():
    manager.clear_all()
    return {"message": "Cleared"}