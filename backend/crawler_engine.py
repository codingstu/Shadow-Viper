# backend/crawler_engine.py
import asyncio
import json
import requests
import re
import pandas as pd
import time
import random
import os
from urllib.parse import quote, unquote, urlparse, urljoin
from fastapi import APIRouter, Response, Request
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from dotenv import load_dotenv
from abc import ABC, abstractmethod

# å¼•å…¥ä»£ç†æ± ç®¡ç†å™¨
try:
    from proxy_engine import manager as pool_manager
except ImportError:
    pool_manager = None

load_dotenv()

router = APIRouter(tags=["crawler"])

# ==================== å…¨å±€é…ç½® ====================
GLOBAL_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
GLOBAL_COOKIE_JAR = {}

VIDEO_SITES = [
    "missav", "missav.ws", "jable", "pornhub", "xvideos",
    "youtube", "youtu.be", "bilibili", "spankbang", "ddh", "jav", "hqporner"
]


class CrawlRequest(BaseModel):
    url: str
    mode: str = "auto"


def random_delay():
    time.sleep(random.uniform(0.5, 1.5))


def is_video_site(url: str) -> bool:
    return any(site in url.lower() for site in VIDEO_SITES)


# ==================== æ ¸å¿ƒç½‘ç»œå·¥å…· ====================
def request_with_chain(url, headers=None, stream=False, timeout=10, method="GET"):
    """é€šç”¨è¯·æ±‚å†…æ ¸ï¼šè‡ªåŠ¨è½®è¯¢ Hunter > Paid > Tor > Direct"""
    if headers is None: headers = {}
    headers.setdefault("User-Agent", GLOBAL_USER_AGENT)

    domain = urlparse(url).netloc
    cookies = GLOBAL_COOKIE_JAR.get(domain)

    # è·å–æ ‡å‡†é“¾è·¯
    chain = []
    if pool_manager:
        chain = pool_manager.get_standard_chain()
    chain.append((None, "Direct", 5))

    last_error = None

    for proxy_url, name, time_limit in chain:
        proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
        try:
            current_timeout = timeout if not proxy_url else time_limit + 5
            resp = requests.request(
                method, url, headers=headers, cookies=cookies, stream=stream,
                timeout=current_timeout, proxies=proxies, verify=False
            )
            # 412 æ˜¯ Bç«™é£æ§ï¼Œè§†ä¸ºå¤±è´¥ç»§ç»­æ¢ IP
            if resp.status_code in [200, 206, 302]:
                resp.network_name = name
                return resp
            elif resp.status_code in [403, 412, 429]:
                last_error = f"{name} Blocked ({resp.status_code})"
                continue
        except Exception as e:
            last_error = str(e)
            continue

    dummy = requests.Response()
    dummy.status_code = 500
    dummy.network_name = f"All Failed: {last_error}"
    return dummy


def parse_playwright_proxy(p_url):
    """Playwright ä»£ç†æ ¼å¼è½¬æ¢"""
    if not p_url: return None
    try:
        u = urlparse(p_url)
        return {"server": f"{u.scheme}://{u.hostname}:{u.port}", "username": u.username, "password": u.password}
    except:
        return None


# ==================== çˆ¬è™«åŸºç±» ====================
class BaseCrawler(ABC):
    @abstractmethod
    async def crawl(self, url: str):
        pass

    async def get_proxy_chain(self):
        chain = []
        if pool_manager: chain = pool_manager.get_standard_chain()
        chain.append((None, "Direct", 10))
        return chain


# ==================== 1. Bç«™ ä¸“ç”¨çˆ¬è™« (ä¿®å¤ 412 é£æ§) ====================
class BilibiliCrawler(BaseCrawler):
    def fetch_api_metadata(self, url):
        """Bç«™ API æ ¸å¿ƒé€»è¾‘ (å¸¦é£æ§æ£€æµ‹)"""
        try:
            bvid_match = re.search(r"(BV\w+)", url)
            if not bvid_match: return None
            bvid = bvid_match.group(1)

            headers = {"User-Agent": GLOBAL_USER_AGENT, "Referer": "https://www.bilibili.com/"}

            # 1. å…ƒæ•°æ® (å¦‚æœè¿”å› 412ï¼Œrequest_with_chain ä¼šå°è¯•æ¢ä»£ç†ï¼Œå¦‚æœéƒ½å¤±è´¥åˆ™è¿”å› 500)
            info_resp = request_with_chain(f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}",
                                           headers=headers)
            if info_resp.status_code != 200: return None  # å¯èƒ½æ˜¯ 412 é£æ§

            data = info_resp.json().get('data')
            if not data: return None

            cid = data['cid']
            meta = {"title": data['title'], "cover": data['pic']}

            # 2. è§†é¢‘æµ (ä¼˜å…ˆ MP4)
            play_api = f"https://api.bilibili.com/x/player/playurl?bvid={bvid}&cid={cid}&qn=64&fnval=1&platform=html5&high_quality=1"
            play_resp = request_with_chain(play_api, headers=headers)

            video_url = ""
            if play_resp.status_code == 200:
                p_data = play_resp.json()
                if p_data['code'] == 0 and 'durl' in p_data['data']:
                    video_url = p_data['data']['durl'][0]['url']

            # 3. ä¸¥æ ¼æ ¸éªŒ (ç¡®ä¿ä»£ç†èƒ½æ‹‰åŠ¨æµ)
            if video_url:
                v_headers = headers.copy()
                v_headers['Range'] = 'bytes=0-100'  # åªè¯»å‰100å­—èŠ‚
                check = request_with_chain(video_url, headers=v_headers, timeout=8)

                # 200 æˆ– 206 å‡è¡¨ç¤ºè¿æ¥æˆåŠŸ
                if check.status_code in [200, 206]:
                    return {**meta, "video_url": video_url, "verified": True}

            return {**meta, "video_url": "", "verified": False}
        except Exception as e:
            print(f"Bili API Error: {e}")
            return None

    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸ“º å¯åŠ¨ Bç«™ ä¸“ç”¨çˆ¬è™«..."}) + "\n"

        # ç­–ç•¥ A: API ç§’å¼€ (å°è¯•è·å–ç›´é“¾)
        api_data = self.fetch_api_metadata(url)
        if api_data and api_data.get('verified'):
            yield json.dumps({"step": "process", "message": f"âœ… API è§£ææˆåŠŸ: {api_data['title'][:15]}..."}) + "\n"
            results = [
                {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": api_data['title'], "å¤‡æ³¨": "API-Title"},
                {"ç±»å‹": "å›¾ç‰‡", "å†…å®¹": api_data['cover'], "å¤‡æ³¨": "Cover"},
                {"ç±»å‹": "è§†é¢‘", "å†…å®¹": api_data['video_url'], "å¤‡æ³¨": "Direct-Stream"}
            ]
            yield pd.DataFrame(results)
            return

        yield json.dumps(
            {"step": "process", "message": "âš ï¸ API å‡è¢«é£æ§ (412) æˆ–æ— æ•ˆï¼Œé™çº§åˆ° Playwright æ¨¡æ‹ŸçœŸäºº..."}) + "\n"

        # ç­–ç•¥ B: æµè§ˆå™¨å—…æ¢ (å¼ºåŠ›å¯¹æŠ— 412)
        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å¯åŠ¨å—…æ¢: [{name}]..."}) + "\n"

            async with async_playwright() as p:
                try:
                    # Bç«™éœ€è¦æœ‰å¤´æ¨¡å¼æ¥åŠ è½½æ’­æ”¾å™¨
                    browser = await p.chromium.launch(headless=False, args=["--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                    page = await context.new_page()

                    captured = []
                    # ç›‘å¬æ‰€æœ‰å¯èƒ½çš„æµåª’ä½“æ ¼å¼
                    page.on("request", lambda r: captured.append(r.url) if any(
                        x in r.url for x in [".m4s", ".flv", ".mp4"]) else None)

                    try:
                        await page.goto(url, timeout=45000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue

                    # è‡ªåŠ¨ç‚¹å‡»æ’­æ”¾ (å¯¹æŠ—æ‡’åŠ è½½)
                    await asyncio.sleep(3)
                    try:
                        await page.click(".bilibili-player-video-wrap", timeout=2000)
                    except:
                        pass

                    # å†ç­‰ä¸€ä¼šç¡®ä¿æµå¼€å§‹ä¼ è¾“
                    await asyncio.sleep(4)

                    # æå– Cookie ä¾›åç»­ä»£ç†æ’­æ”¾ä½¿ç”¨
                    cookies = await context.cookies()
                    GLOBAL_COOKIE_JAR[urlparse(url).netloc] = {c['name']: c['value'] for c in cookies}

                    # ä¼˜å…ˆå–æœ€é•¿çš„ URL (é€šå¸¸æ˜¯é«˜ç”»è´¨)
                    final_video = max(captured, key=len) if captured else ""

                    if final_video:
                        title = await page.title()
                        yield json.dumps({"step": "process", "message": f"âœ… å—…æ¢æˆåŠŸ..."}) + "\n"
                        yield pd.DataFrame([
                            {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title.strip(), "å¤‡æ³¨": "Sniff-Title"},
                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}
                        ])
                        await browser.close();
                        return

                    await browser.close()
                except:
                    continue

        yield json.dumps({"step": "error", "message": "âŒ Bç«™ æ‰€æœ‰é€šé“å°è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ›´æ–°ä»£ç†æ± "}) + "\n"


# ==================== 2. YouTube ä¸“ç”¨çˆ¬è™« (æ¨¡æ‹Ÿ iOS) ====================
class YouTubeCrawler(BaseCrawler):
    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸŸ¥ å¯åŠ¨ YouTube ä¸“ç”¨çˆ¬è™« (iOS ä¼ªè£…)..."}) + "\n"

        # å®šä¹‰ iOS UA
        MOBILE_UA = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"

        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å°è¯•èŠ‚ç‚¹: {name}..."}) + "\n"

            async with async_playwright() as p:
                try:
                    # å¯åŠ¨æ¨¡æ‹Ÿæ‰‹æœºçš„æµè§ˆå™¨
                    browser = await p.chromium.launch(
                        headless=False,
                        args=["--disable-blink-features=AutomationControlled", "--mute-audio"],
                        proxy=proxy_conf
                    )
                    context = await browser.new_context(
                        user_agent=MOBILE_UA,  # å…³é”®ï¼šçˆ¬è™«ä¹Ÿæ˜¯è¿™ä¸ª UA
                        viewport={"width": 375, "height": 812},
                        is_mobile=True,
                        has_touch=True,
                        ignore_https_errors=True
                    )
                    page = await context.new_page()

                    captured = []
                    # ç›‘å¬ m3u8 å’Œ videoplayback
                    page.on("request", lambda r: captured.append(r.url) if any(
                        k in r.url for k in ["videoplayback", ".m3u8"]) else None)

                    try:
                        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue

                    # è‡ªåŠ¨ç‚¹å‡»ç§»åŠ¨ç«¯å¼¹çª—
                    await asyncio.sleep(2)
                    try:
                        if await page.is_visible("button[aria-label='Reject all']"):
                            await page.click("button[aria-label='Reject all']")
                        # ç§»åŠ¨ç«¯é€šå¸¸éœ€è¦ç‚¹å‡»ä¸€ä¸‹å±å¹•ä¸­é—´æ¥æ’­æ”¾
                        await page.tap("#player-container-id")
                        await page.tap(".html5-main-video")
                    except:
                        pass

                    await asyncio.sleep(5)

                    final_video = ""
                    # ä¼˜å…ˆ m3u8 (HLS)ï¼Œå…¶æ¬¡ videoplayback
                    m3u8_list = [u for u in captured if "m3u8" in u]
                    if m3u8_list:
                        final_video = m3u8_list[0]
                    elif captured:
                        # è¿‡æ»¤æ‰ä»…éŸ³é¢‘æµ
                        candidates = [u for u in captured if "mime=audio" not in u]
                        final_video = max(candidates, key=len) if candidates else captured[0]

                    if final_video:
                        title = await page.title()
                        clean_title = title.replace(" - YouTube", "").strip()
                        yield json.dumps({"step": "process", "message": f"âœ… YouTube æ•è·æˆåŠŸ (HLS/MP4)..."}) + "\n"
                        yield pd.DataFrame([
                            {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": clean_title, "å¤‡æ³¨": "Title"},
                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}
                        ])
                        await browser.close();
                        return

                    await browser.close()
                except:
                    continue

        yield json.dumps({"step": "error", "message": "âŒ YouTube ä»»åŠ¡å¤±è´¥"}) + "\n"


# ==================== 3. é€šç”¨è§†é¢‘çˆ¬è™« (MissAV ç­‰) ====================
class UniversalVideoCrawler(BaseCrawler):
    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸ¬ å¯åŠ¨é€šç”¨è§†é¢‘å—…æ¢..."}) + "\n"

        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å¯åŠ¨æµè§ˆå™¨: [{name}]..."}) + "\n"

            async with async_playwright() as p:
                try:
                    browser = await p.chromium.launch(headless=False, args=["--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                    page = await context.new_page()

                    captured = []
                    # é€šç”¨å—…æ¢è§„åˆ™ï¼šm3u8, mp4
                    page.on("request", lambda r: captured.append(r.url) if any(
                        x in r.url for x in [".m3u8", ".mp4"]) and not r.url.startswith("blob:") else None)

                    try:
                        await page.goto(url, timeout=45000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue

                    await asyncio.sleep(3)
                    # å°è¯•ç‚¹å‡»æ’­æ”¾ (é€šç”¨é€‰æ‹©å™¨)
                    try:
                        await page.click("video, .player", timeout=2000)
                    except:
                        pass
                    await asyncio.sleep(3)

                    # æ•è· Cookie (MissAV å¿…é¡»)
                    cookies = await context.cookies()
                    domain = urlparse(url).netloc
                    GLOBAL_COOKIE_JAR[domain] = {c['name']: c['value'] for c in cookies}
                    if "missav" in domain: GLOBAL_COOKIE_JAR["missav.ws"] = GLOBAL_COOKIE_JAR[domain]

                    # ä¼˜é€‰ m3u8
                    priority = [u for u in captured if ".m3u8" in u]
                    final_video = priority[0] if priority else (captured[0] if captured else "")

                    if not final_video:
                        if v := await page.query_selector("video"):
                            src = await v.get_attribute("src")
                            if src and src.startswith("http"): final_video = src

                    if final_video:
                        title = await page.title()
                        yield json.dumps({"step": "process", "message": f"âœ… å—…æ¢æˆåŠŸ..."}) + "\n"
                        yield pd.DataFrame([
                            {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title.strip(), "å¤‡æ³¨": "Title"},
                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}
                        ])
                        await browser.close();
                        return

                    await browser.close()
                except:
                    continue

        yield json.dumps({"step": "error", "message": "âŒ æœªå—…æ¢åˆ°è§†é¢‘æµ"}) + "\n"


# ==================== 4. æé€Ÿæ–‡æœ¬çˆ¬è™« ====================
class GeneralTextCrawler(BaseCrawler):
    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸš€ å¯åŠ¨æé€Ÿæ–‡æœ¬è§£æ..."}) + "\n"
        random_delay()

        resp = request_with_chain(url)
        net_name = getattr(resp, "network_name", "æœªçŸ¥")

        if resp.status_code != 200:
            yield json.dumps({"step": "error", "message": f"âŒ è¯·æ±‚å¤±è´¥: {net_name} ({resp.status_code})"}) + "\n"
            return

        yield json.dumps({"step": "process", "message": f"ğŸŒ é€šé“: {net_name}"}) + "\n"

        if len(resp.text) < 500:
            yield json.dumps({"step": "process", "message": "âš ï¸ é¡µé¢å†…å®¹è¿‡çŸ­"}) + "\n"

        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        data_list = []

        title = soup.title.string.strip() if soup.title else ""
        if title: data_list.append({"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title, "å¤‡æ³¨": "Meta-Title"})

        article = soup.find("div", class_="RichText") or soup.find("div", class_="markdown-body") or soup.find(
            "article") or soup.body
        if article:
            for tag in article(
                    ["script", "style", "noscript", "svg", "button", "input", "form", "nav", "footer", "iframe"]):
                tag.decompose()

            for tag in article.find_all(['p', 'h1', 'h2', 'h3', 'li']):
                txt = tag.get_text(strip=True)
                if len(txt) > 5:
                    data_list.append({"ç±»å‹": tag.name.upper(), "å†…å®¹": txt, "å¤‡æ³¨": "Text"})

        if data_list:
            yield pd.DataFrame(data_list)
        else:
            yield json.dumps({"step": "error", "message": "âŒ æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬"}) + "\n"


# ==================== å·¥å‚ç±» & è·¯ç”± ====================
class CrawlerFactory:
    @staticmethod
    def get_crawler(url: str, mode: str) -> BaseCrawler:
        is_video = is_video_site(url)

        if "bilibili.com" in url:
            return BilibiliCrawler()
        elif "youtube.com" in url or "youtu.be" in url:
            return YouTubeCrawler()
        elif is_video or mode == "media":
            return UniversalVideoCrawler()
        else:
            return GeneralTextCrawler()


async def smart_router(url: str, mode: str):
    yield json.dumps({"step": "init", "message": f"ä»»åŠ¡å¯åŠ¨: {url} [Mode: {mode}]"}) + "\n"
    await asyncio.sleep(0.5)

    crawler = CrawlerFactory.get_crawler(url, mode)

    try:
        df = pd.DataFrame()
        async for chunk in crawler.crawl(url):
            if isinstance(chunk, pd.DataFrame):
                df = pd.concat([df, chunk], ignore_index=True)
            else:
                yield chunk

        if not df.empty:
            filename = f"data_{int(time.time())}.csv"
            filepath = os.path.abspath(filename)
            df.to_csv(filepath, index=False, encoding="utf-8-sig")
            download_url = f"http://127.0.0.1:8000/download/{filename}"
            yield json.dumps({"step": "done", "download_url": download_url}) + "\n"
        else:
            yield json.dumps({"step": "error", "message": "ç»“æœä¸ºç©º"}) + "\n"

    except Exception as e:
        yield json.dumps({"step": "error", "message": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"}) + "\n"


# ==================== è§†é¢‘æµä»£ç† (Referer å¢å¼ºç‰ˆ) ====================
@router.get("/api/proxy")
# ==================== è§†é¢‘æµä»£ç† (YouTube ä¸“ç”¨ä¼˜åŒ–) ====================

# ==================== è§†é¢‘æµä»£ç† (ä¿®å¤ YouTube æ’­æ”¾) ====================
@router.get("/api/proxy")
async def proxy_stream(url: str, request: Request):
    target_url = unquote(url)
    parsed = urlparse(target_url)
    domain = parsed.netloc.lower()

    # é»˜è®¤ PC UA
    current_ua = GLOBAL_USER_AGENT

    # ç­–ç•¥é…ç½®
    referer = f"{parsed.scheme}://{domain}/"
    origin = f"{parsed.scheme}://{domain}"

    if "bili" in domain:
        referer = "https://www.bilibili.com/"
    elif "missav" in domain or "surrit" in domain:
        referer = "https://missav.ws/"
        origin = "https://missav.ws"
    # ğŸ”¥ğŸ”¥ğŸ”¥ YouTube ä¸“ç”¨ä¿®å¤ ğŸ”¥ğŸ”¥ğŸ”¥
    elif "googlevideo" in domain or "youtube" in domain:
        referer = "https://www.youtube.com/"
        origin = "https://www.youtube.com"
        # å…³é”®ï¼šå¿…é¡»å’Œçˆ¬è™«ä¸€æ ·ä½¿ç”¨ iOS UAï¼Œå¦åˆ™ Google ä¼šæ‹’ç»è¿æ¥
        current_ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"

    headers = {
        "User-Agent": current_ua,
        "Referer": referer,
        "Origin": origin,
        "Range": request.headers.get("range", "bytes=0-")
    }

    # ä»£ç†é“¾ç­–ç•¥
    proxy_chain = []

    # YouTube è§†é¢‘æµé€šå¸¸ç»‘å®š IPï¼Œç›´è¿æˆåŠŸç‡åè€Œæœ€é«˜ï¼ˆå¦‚æœæœåŠ¡å™¨åœ¨æµ·å¤–ï¼‰
    # æˆ–è€…å¿…é¡»ä½¿ç”¨æå…¶ç¨³å®šçš„æ¢¯å­ã€‚è¿™é‡Œå°è¯• ç›´è¿ -> ä»£ç†æ± 
    if "googlevideo" in domain:
        proxy_chain.append((None, "Direct Priority", 5))
        if pool_manager: proxy_chain.extend(pool_manager.get_standard_chain())
    else:
        if pool_manager: proxy_chain = pool_manager.get_standard_chain()
        proxy_chain.append((None, "Direct", 10))

    for proxy_url, name, timeout_sec in proxy_chain:
        try:
            proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
            session = requests.Session()
            resp = session.get(
                target_url, headers=headers, cookies=GLOBAL_COOKIE_JAR.get(domain),
                stream=True, timeout=(5, timeout_sec), verify=False, proxies=proxies
            )

            # è¿‡æ»¤é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯ 403 (Google å¸¸è¿”å› 403 ä»£è¡¨é“¾æ¥å¤±æ•ˆæˆ– IP ä¸å¯¹)
            if resp.status_code >= 400:
                continue

            # è¿‡æ»¤ HTML (æœ‰æ—¶å€™ä»£ç†ä¼šè¿”å›ç™»å½•é¡µ)
            content_type = resp.headers.get("content-type", "application/octet-stream")
            if "text/html" in content_type:
                continue

            # M3U8 ä»£ç†é‡å†™ (å…³é”®ï¼šè®©åˆ†ç‰‡ä¹Ÿèµ°è¿™ä¸ªä»£ç†æ¥å£)
            if "mpegurl" in content_type or ".m3u8" in target_url:
                new_lines = []
                for line in resp.text.splitlines():
                    line = line.strip()
                    if not line or line.startswith("#"):
                        new_lines.append(line)
                    else:
                        # å°† m3u8 é‡Œçš„ç›¸å¯¹/ç»å¯¹è·¯å¾„éƒ½è½¬å›æˆ‘ä»¬çš„ä»£ç†æ¥å£
                        full_ts_url = urljoin(target_url, line)
                        proxy_ts_url = f"http://127.0.0.1:8000/api/proxy?url={quote(full_ts_url)}"
                        new_lines.append(proxy_ts_url)

                return Response(content="\n".join(new_lines), media_type=content_type)

            # æ™®é€šæµåª’ä½“é€ä¼ 
            return StreamingResponse(
                resp.iter_content(chunk_size=64 * 1024),
                status_code=resp.status_code,
                headers={
                    "Content-Type": content_type,
                    "Content-Range": resp.headers.get("Content-Range"),
                    "Content-Length": resp.headers.get("Content-Length"),
                    "Accept-Ranges": "bytes"
                },
                media_type=content_type
            )
        except:
            continue

    return Response(status_code=502, content="Stream Failed")


@router.post("/api/crawl")
async def start_crawl(request: CrawlRequest):
    return StreamingResponse(smart_router(request.url, request.mode), media_type="application/x-ndjson")


@router.get("/download/{filename}")
async def download_file(filename: str):
    filepath = os.path.abspath(filename)
    if os.path.exists(filepath): return FileResponse(filepath, filename=filename)
    return Response("File not found", status_code=404)