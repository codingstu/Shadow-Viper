# backend/crawler_engine.py
import asyncio
import json
import re
import pandas as pd
import time
import random
import os
import requests  # ğŸ”¥ åšå®šå›å½’ Requests
from urllib.parse import quote, unquote, urlparse, urljoin
from fastapi import APIRouter, Response, Request
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Route
from dotenv import load_dotenv
from abc import ABC, abstractmethod

# å¼•å…¥ä»£ç†æ± ç®¡ç†å™¨
try:
    from proxy_engine import manager as pool_manager
except ImportError:
    pool_manager = None

load_dotenv()

router = APIRouter(tags=["crawler"])

# ==================== å…¨å±€é…ç½® ====================
# ğŸ”¥ å‡çº§ï¼šæ›´çœŸå®çš„æµè§ˆå™¨æŒ‡çº¹ï¼Œå¯¹æŠ— 403
GLOBAL_USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
GLOBAL_COOKIE_JAR = {}

VIDEO_SITES = [
    "missav", "missav.ws", "jable", "pornhub", "xvideos",
    "youtube", "youtu.be", "bilibili", "spankbang", "ddh", "jav", "hqporner"
]


class CrawlRequest(BaseModel):
    url: str
    mode: str = "auto"


def random_delay():
    time.sleep(random.uniform(0.5, 1.5))


def is_video_site(url: str) -> bool:
    return any(site in url.lower() for site in VIDEO_SITES)


# ==================== ğŸš€ æ ¸å¿ƒä¿®å¤ï¼šå¼‚æ­¥çº¿ç¨‹åŒ…è£¹ Requests ====================
async def async_request(method, url, **kwargs):
    """
    ğŸ”¥ é­”æ³•å‡½æ•°ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ä½¿ç”¨ requests è€Œä¸å¡æ­»æœåŠ¡å™¨
    åŸç†ï¼šå°†åŒæ­¥çš„ requests æ“ä½œæ‰”åˆ°çº¿ç¨‹æ± ä¸­è¿è¡Œ
    """
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: requests.request(method, url, **kwargs))


async def request_with_chain_async(url, headers=None, stream=False, timeout=10, method="GET"):
    """
    åŸºäº requests çš„å¼‚æ­¥ä»£ç†é“¾è¯·æ±‚
    """
    if headers is None: headers = {}

    # ğŸ”¥ å…³é”®ï¼šè¡¥å…¨é«˜ä»¿æµè§ˆå™¨ Headers (è§£å†³ linux.do 403 é—®é¢˜)
    base_headers = {
        "User-Agent": GLOBAL_USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1"
    }
    base_headers.update(headers)

    domain = urlparse(url).netloc
    cookies = GLOBAL_COOKIE_JAR.get(domain)

    # è·å–æ ‡å‡†é“¾è·¯ (Hunter -> Paid -> Tor -> Direct)
    chain = []
    if pool_manager:
        chain = pool_manager.get_standard_chain()

    # æœ€åæ‰åŠ  Direct (ç›´è¿)
    chain.append((None, "Direct", 5))

    last_error = None
    failed_count = 0

    for proxy_url, name, time_limit in chain:
        try:
            current_timeout = timeout if not proxy_url else time_limit + 5

            # æ„é€  requests æ ¼å¼ä»£ç†å­—å…¸
            proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None

            # ğŸ”¥ æ”¾åœ¨çº¿ç¨‹æ± è·‘ï¼Œä¸é˜»å¡
            resp = await async_request(
                method,
                url,
                headers=base_headers,
                cookies=cookies,
                proxies=proxies,
                timeout=current_timeout,
                verify=False,  # å¿½ç•¥ SSL æŠ¥é”™
                stream=stream
            )

            # æˆåŠŸåˆ¤å®šçš„çŠ¶æ€ç 
            if resp.status_code in [200, 206, 302]:
                resp.network_name = name
                return resp
            elif resp.status_code in [403, 412, 429]:
                # é‡åˆ°é£æ§ï¼Œè®°å½•é”™è¯¯ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
                last_error = f"{name} Blocked ({resp.status_code})"
                failed_count += 1
                continue
            else:
                last_error = f"{name} Error ({resp.status_code})"
                failed_count += 1
                continue

        except Exception as e:
            last_error = f"{name} Exception: {str(e)}"
            failed_count += 1
            continue

    # æ„é€ å¤±è´¥å“åº”
    class DummyResponse:
        status_code = 500
        text = ""
        content = b""
        network_name = f"Failed ({failed_count} paths tried). Last: {last_error}"

        def json(self): return {}

    return DummyResponse()


def parse_playwright_proxy(p_url):
    if not p_url: return None
    try:
        u = urlparse(p_url)
        return {"server": f"{u.scheme}://{u.hostname}:{u.port}", "username": u.username, "password": u.password}
    except:
        return None


# ==================== è¾…åŠ©å‡½æ•°ï¼šèµ„æºæ‹¦æˆª ====================
async def block_media_and_images(route: Route):
    """æ‹¦æˆªå›¾ç‰‡å’Œå­—ä½“ï¼ŒåŠ é€Ÿé¡µé¢åŠ è½½"""
    if route.request.resource_type in ["image", "font"]:
        await route.abort()
    else:
        await route.continue_()

async def block_aggressive(route: Route):
    """æ¿€è¿›æ‹¦æˆªï¼šæ‹¦æˆªå›¾ç‰‡ã€å­—ä½“ã€åª’ä½“å’Œæ ·å¼è¡¨ï¼ˆç”¨äºçº¯æ–‡æœ¬æå–ï¼‰"""
    if route.request.resource_type in ["image", "font", "media", "stylesheet"]:
        await route.abort()
    else:
        await route.continue_()


# ==================== çˆ¬è™«åŸºç±» ====================
class BaseCrawler(ABC):
    @abstractmethod
    async def crawl(self, url: str):
        pass

    async def get_proxy_chain(self):
        chain = []
        if pool_manager: chain = pool_manager.get_standard_chain()
        chain.append((None, "Direct", 10))
        return chain


# ==================== 1. Bç«™ ä¸“ç”¨çˆ¬è™« ====================
class BilibiliCrawler(BaseCrawler):
    async def fetch_api_metadata_async(self, url):
        try:
            bvid_match = re.search(r"(BV\w+)", url)
            if not bvid_match: return None
            bvid = bvid_match.group(1)

            # ğŸ”¥ ä¿®å¤ 1: ä½¿ç”¨å…·ä½“çš„è§†é¢‘é¡µä½œä¸º Referer
            headers = {"Referer": f"https://www.bilibili.com/video/{bvid}"}

            # ğŸ”¥ ä¿®å¤ 2: å…ˆè®¿é—®ä¸€æ¬¡ä¸»é¡µè·å– Cookie (è¿™ä¸€æ­¥è‡³å…³é‡è¦)
            # Bç«™ API éœ€è¦ buvid3 ç­‰ cookie æ‰èƒ½æ­£å¸¸è¿”å›æ•°æ®
            await request_with_chain_async(url, headers=headers, method="HEAD")

            # 1. å…ƒæ•°æ®
            info_resp = await request_with_chain_async(f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}",
                                                       headers=headers)
            if info_resp.status_code != 200: return None

            data = info_resp.json().get('data')
            if not data: return None

            cid = data['cid']
            meta = {"title": data['title'], "cover": data['pic']}

            # 2. è§†é¢‘æµ
            play_api = f"https://api.bilibili.com/x/player/playurl?bvid={bvid}&cid={cid}&qn=64&fnval=1&platform=html5&high_quality=1"
            play_resp = await request_with_chain_async(play_api, headers=headers)

            video_url = ""
            if play_resp.status_code == 200:
                p_data = play_resp.json()
                if p_data['code'] == 0 and 'durl' in p_data['data']:
                    video_url = p_data['data']['durl'][0]['url']

            # 3. ä¸¥æ ¼æ ¸éªŒ
            if video_url:
                v_headers = headers.copy()
                v_headers['Range'] = 'bytes=0-100'
                check = await request_with_chain_async(video_url, headers=v_headers, timeout=8)
                if check.status_code in [200, 206]:
                    return {**meta, "video_url": video_url, "verified": True}

            return {**meta, "video_url": "", "verified": False}
        except Exception as e:
            return None

    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸ“º å¯åŠ¨ Bç«™ ä¸“ç”¨çˆ¬è™«..."}) + "\n"

        api_data = await self.fetch_api_metadata_async(url)
        if api_data and api_data.get('verified'):
            yield json.dumps({"step": "process", "message": f"âœ… API è§£ææˆåŠŸ: {api_data['title'][:15]}..."}) + "\n"
            results = [
                {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": api_data['title'], "å¤‡æ³¨": "API-Title"},
                {"ç±»å‹": "å›¾ç‰‡", "å†…å®¹": api_data['cover'], "å¤‡æ³¨": "Cover"},
                {"ç±»å‹": "è§†é¢‘", "å†…å®¹": api_data['video_url'], "å¤‡æ³¨": "Direct-Stream"}
            ]
            yield pd.DataFrame(results)
            return

        yield json.dumps({"step": "process", "message": "âš ï¸ API å—é™ï¼Œå¯åŠ¨ Playwright å—…æ¢..."}) + "\n"

        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å¯åŠ¨æµè§ˆå™¨: [{name}]..."}) + "\n"

            async with async_playwright() as p:
                try:
                    # Bç«™å¯èƒ½éœ€è¦ headful æ¨¡å¼æ¥é€šè¿‡æŸäº›æ£€æŸ¥
                    browser = await p.chromium.launch(headless=False, args=["--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                    page = await context.new_page()

                    # ğŸ”¥ ä¿®å¤ï¼šBç«™è§†é¢‘æµå—…æ¢ä¸èƒ½æ‹¦æˆª mediaï¼Œå¦åˆ™æ— æ³•æ•è· .m4s/.flv
                    # ä»…æ‹¦æˆªå›¾ç‰‡å’Œå­—ä½“
                    await page.route("**/*.{png,jpg,jpeg,gif,webp,svg,woff,woff2,ttf,otf}", lambda route: route.abort())

                    captured = []
                    page.on("request", lambda r: captured.append(r.url) if any(
                        x in r.url for x in [".m4s", ".flv", ".mp4"]) else None)

                    try:
                        await page.goto(url, timeout=45000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue

                    await asyncio.sleep(3)
                    try:
                        await page.click(".bilibili-player-video-wrap", timeout=2000)
                    except:
                        pass
                    await asyncio.sleep(4)

                    cookies = await context.cookies()
                    GLOBAL_COOKIE_JAR[urlparse(url).netloc] = {c['name']: c['value'] for c in cookies}

                    final_video = max(captured, key=len) if captured else ""

                    if final_video:
                        title = await page.title()
                        yield json.dumps({"step": "process", "message": f"âœ… å—…æ¢æˆåŠŸ..."}) + "\n"
                        yield pd.DataFrame([
                            {"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title.strip(), "å¤‡æ³¨": "Sniff-Title"},
                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}
                        ])
                        await browser.close();
                        return
                    await browser.close()
                except:
                    continue

        yield json.dumps({"step": "error", "message": "âŒ Bç«™ ä»»åŠ¡å¤±è´¥"}) + "\n"


# ==================== 2. YouTube ä¸“ç”¨çˆ¬è™« ====================
class YouTubeCrawler(BaseCrawler):
    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸŸ¥ å¯åŠ¨ YouTube çˆ¬è™«..."}) + "\n"
        MOBILE_UA = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å°è¯•èŠ‚ç‚¹: {name}..."}) + "\n"
            async with async_playwright() as p:
                try:
                    browser = await p.chromium.launch(headless=False,
                                                      args=["--disable-blink-features=AutomationControlled",
                                                            "--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=MOBILE_UA, viewport={"width": 375, "height": 812},
                                                        is_mobile=True, has_touch=True)
                    page = await context.new_page()

                    # YouTube æ¯”è¾ƒæ•æ„Ÿï¼Œåªæ‹¦æˆªå­—ä½“ï¼Œä¿ç•™å›¾ç‰‡å¯èƒ½æœ‰åŠ©äºåŠ è½½
                    await page.route("**/*.{woff,woff2,ttf,otf}", lambda route: route.abort())

                    captured = []
                    page.on("request", lambda r: captured.append(r.url) if any(
                        k in r.url for k in ["videoplayback", ".m3u8"]) else None)
                    try:
                        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue
                    await asyncio.sleep(2)
                    try:
                        if await page.is_visible("button[aria-label='Reject all']"): await page.click(
                            "button[aria-label='Reject all']")
                        await page.tap("#player-container-id");
                        await page.tap(".html5-main-video")
                    except:
                        pass
                    await asyncio.sleep(5)
                    final_video = ""
                    m3u8_list = [u for u in captured if "m3u8" in u]
                    if m3u8_list:
                        final_video = m3u8_list[0]
                    elif captured:
                        candidates = [u for u in captured if "mime=audio" not in u]
                        final_video = max(candidates, key=len) if candidates else captured[0]
                    if final_video:
                        title = await page.title()
                        clean_title = title.replace(" - YouTube", "").strip()
                        yield json.dumps({"step": "process", "message": f"âœ… æ•è·æˆåŠŸ..."}) + "\n"
                        yield pd.DataFrame([{"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": clean_title, "å¤‡æ³¨": "Title"},
                                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}])
                        await browser.close();
                        return
                    await browser.close()
                except:
                    continue
        yield json.dumps({"step": "error", "message": "âŒ YouTube ä»»åŠ¡å¤±è´¥"}) + "\n"


# ==================== 3. é€šç”¨è§†é¢‘çˆ¬è™« ====================
class UniversalVideoCrawler(BaseCrawler):
    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸ¬ å¯åŠ¨é€šç”¨è§†é¢‘å—…æ¢..."}) + "\n"
        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ å¯åŠ¨æµè§ˆå™¨: [{name}]..."}) + "\n"
            async with async_playwright() as p:
                try:
                    browser = await p.chromium.launch(headless=False, args=["--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                    page = await context.new_page()

                    # æ‹¦æˆªå›¾ç‰‡å’Œå­—ä½“
                    await page.route("**/*", block_media_and_images)

                    captured = []
                    page.on("request", lambda r: captured.append(r.url) if any(
                        x in r.url for x in [".m3u8", ".mp4"]) and not r.url.startswith("blob:") else None)
                    try:
                        await page.goto(url, timeout=45000, wait_until="domcontentloaded")
                    except:
                        await browser.close(); continue
                    await asyncio.sleep(3)
                    try:
                        await page.click("video, .player", timeout=2000)
                    except:
                        pass
                    await asyncio.sleep(3)
                    cookies = await context.cookies()
                    domain = urlparse(url).netloc
                    GLOBAL_COOKIE_JAR[domain] = {c['name']: c['value'] for c in cookies}
                    if "missav" in domain: GLOBAL_COOKIE_JAR["missav.ws"] = GLOBAL_COOKIE_JAR[domain]
                    priority = [u for u in captured if ".m3u8" in u]
                    final_video = priority[0] if priority else (captured[0] if captured else "")
                    if not final_video:
                        if v := await page.query_selector("video"):
                            src = await v.get_attribute("src")
                            if src and src.startswith("http"): final_video = src
                    if final_video:
                        title = await page.title()
                        yield json.dumps({"step": "process", "message": f"âœ… å—…æ¢æˆåŠŸ..."}) + "\n"
                        yield pd.DataFrame([{"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title.strip(), "å¤‡æ³¨": "Title"},
                                            {"ç±»å‹": "è§†é¢‘", "å†…å®¹": final_video, "å¤‡æ³¨": "Stream"}])
                        await browser.close();
                        return
                    await browser.close()
                except:
                    continue
        yield json.dumps({"step": "error", "message": "âŒ æœªå—…æ¢åˆ°è§†é¢‘æµ"}) + "\n"


# ==================== 4. æé€Ÿæ–‡æœ¬çˆ¬è™« (Requests ç‰ˆæœ¬) ====================
class GeneralTextCrawler(BaseCrawler):
    def extract_text_from_html(self, html):
        """è¿˜åŸæ—§ç‰ˆé€»è¾‘"""
        soup = BeautifulSoup(html, "html.parser")
        data_list = []

        title = soup.title.string.strip() if soup.title else ""
        if title: data_list.append({"ç±»å‹": "æ ‡é¢˜", "å†…å®¹": title, "å¤‡æ³¨": "Meta-Title"})

        article = soup.find("div", class_="RichText") or \
                  soup.find("div", class_="markdown-body") or \
                  soup.find("article") or \
                  soup.body

        if article:
            for tag in article(
                    ["script", "style", "noscript", "svg", "button", "input", "form", "nav", "footer", "iframe"]):
                tag.decompose()

            for tag in article.find_all(['p', 'h1', 'h2', 'h3', 'li']):
                txt = tag.get_text(strip=True)
                if len(txt) > 5:  # è¿˜åŸé˜ˆå€¼
                    data_list.append({"ç±»å‹": tag.name.upper(), "å†…å®¹": txt, "å¤‡æ³¨": "Text"})
        return data_list

    async def extract_text_async(self, html):
        """ğŸ”¥ ä¼˜åŒ–ï¼šå°† CPU å¯†é›†çš„ BeautifulSoup è§£ææ”¾å…¥çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯"""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self.extract_text_from_html, html)

    async def crawl(self, url: str):
        yield json.dumps({"step": "process", "message": "ğŸš€ å¯åŠ¨æé€Ÿæ–‡æœ¬è§£æ (Requests)..."}) + "\n"
        await asyncio.sleep(random.uniform(0.5, 1.5))

        # 1. å°è¯• requests (çº¿ç¨‹æ± æ‰§è¡Œ)
        resp = await request_with_chain_async(url)
        net_name = getattr(resp, "network_name", "æœªçŸ¥")

        data_list = []
        if resp.status_code == 200:
            resp.encoding = 'utf-8'
            if len(resp.text) > 500:
                # ä½¿ç”¨å¼‚æ­¥åŒ…è£…çš„è§£æå‡½æ•°
                data_list = await self.extract_text_async(resp.text)

        # æˆåŠŸåˆ™ç›´æ¥è¿”å›
        if data_list:
            yield json.dumps({"step": "process", "message": f"ğŸŒ é™æ€æå–æˆåŠŸ ({net_name})"}) + "\n"
            yield pd.DataFrame(data_list)
            return

        # 2. å¦‚æœ requests å¤±è´¥ï¼Œå¯åŠ¨æµè§ˆå™¨å…œåº•
        yield json.dumps({"step": "process", "message": f"âš ï¸ é™æ€æŠ“å–å¤±è´¥ ({net_name})ï¼Œå¯åŠ¨æµè§ˆå™¨æ¸²æŸ“..."}) + "\n"

        chain = await self.get_proxy_chain()
        for proxy_url, name, _ in chain:
            proxy_conf = parse_playwright_proxy(proxy_url)
            yield json.dumps({"step": "process", "message": f"ğŸŒ æ¸²æŸ“èŠ‚ç‚¹: {name}..."}) + "\n"

            async with async_playwright() as p:
                try:
                    # æ–‡æœ¬çˆ¬å–å¯ä»¥ä½¿ç”¨ headless=Trueï¼Œé€Ÿåº¦æ›´å¿«
                    browser = await p.chromium.launch(headless=True, args=["--mute-audio"], proxy=proxy_conf)
                    context = await browser.new_context(user_agent=GLOBAL_USER_AGENT)
                    page = await context.new_page()

                    # ğŸ”¥ ä¼˜åŒ–ï¼šæ¿€è¿›æ‹¦æˆªå›¾ç‰‡ã€å­—ä½“ã€åª’ä½“å’ŒCSSï¼Œæå¤§æå‡åŠ è½½é€Ÿåº¦
                    await page.route("**/*", block_aggressive)

                    try:
                        await page.goto(url, timeout=45000, wait_until="domcontentloaded")
                        await asyncio.sleep(3)
                    except:
                        await browser.close(); continue

                    content = await page.content()
                    # åŒæ ·ä½¿ç”¨å¼‚æ­¥è§£æ
                    data_list = await self.extract_text_async(content)

                    if data_list:
                        yield json.dumps({"step": "process", "message": "âœ… æ·±åº¦æ¸²æŸ“æå–æˆåŠŸ"}) + "\n"
                        yield pd.DataFrame(data_list)
                        await browser.close();
                        return
                    await browser.close()
                except:
                    continue

        yield json.dumps({"step": "error", "message": "âŒ æœ€ç»ˆå¤±è´¥ï¼šæ— æ³•æå–æœ‰æ•ˆæ–‡æœ¬"}) + "\n"


# ==================== å·¥å‚ç±» & è·¯ç”± ====================
class CrawlerFactory:
    @staticmethod
    def get_crawler(url: str, mode: str) -> BaseCrawler:
        if "bilibili.com" in url:
            return BilibiliCrawler()
        elif "youtube.com" in url or "youtu.be" in url:
            return YouTubeCrawler()
        elif is_video_site(url) or mode == "media":
            return UniversalVideoCrawler()
        return GeneralTextCrawler()


async def smart_router(url: str, mode: str):
    yield json.dumps({"step": "init", "message": f"ä»»åŠ¡å¯åŠ¨: {url} [Mode: {mode}]"}) + "\n"
    await asyncio.sleep(0.5)

    crawler = CrawlerFactory.get_crawler(url, mode)

    try:
        df = pd.DataFrame()
        async for chunk in crawler.crawl(url):
            if isinstance(chunk, pd.DataFrame):
                df = pd.concat([df, chunk], ignore_index=True)
            else:
                yield chunk

        if not df.empty:
            filename = f"data_{int(time.time())}.csv"
            filepath = os.path.abspath(filename)
            df.to_csv(filepath, index=False, encoding="utf-8-sig")
            download_url = f"http://127.0.0.1:8000/download/{filename}"
            yield json.dumps({"step": "done", "download_url": download_url}) + "\n"
        else:
            yield json.dumps({"step": "error", "message": "ç»“æœä¸ºç©º"}) + "\n"

    except Exception as e:
        yield json.dumps({"step": "error", "message": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"}) + "\n"


# ==================== è§†é¢‘æµä»£ç† ====================
@router.get("/api/proxy")
async def proxy_stream(url: str, request: Request):
    target_url = unquote(url)
    parsed = urlparse(target_url)
    domain = parsed.netloc.lower()
    current_ua = GLOBAL_USER_AGENT
    referer = f"{parsed.scheme}://{domain}/"
    origin = f"{parsed.scheme}://{domain}"

    if "youtube" in domain:
        current_ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"

    # ğŸª ä¿®å¤ Cookie è·¨åŸŸé—®é¢˜ï¼šbilivideo.com éœ€è¦ä½¿ç”¨ bilibili.com çš„ Cookie
    req_cookies = GLOBAL_COOKIE_JAR.get(domain)
    if not req_cookies and ("bilivideo.com" in domain or "bilibili" in domain):
        # å°è¯•ä»ä¸»ç«™åŸŸåè·å– Cookie
        req_cookies = GLOBAL_COOKIE_JAR.get("www.bilibili.com") or GLOBAL_COOKIE_JAR.get("bilibili.com")

    # ğŸ”¥ ç´§æ€¥ä¿®å¤ï¼šBç«™è§†é¢‘æµé˜²ç›—é“¾å¤„ç†
    if "bilivideo.com" in domain or "bilibili" in domain:
        referer = "https://www.bilibili.com/"
        origin = "https://www.bilibili.com"

    headers = {"User-Agent": current_ua, "Referer": referer, "Origin": origin,
               "Range": request.headers.get("range", "bytes=0-")}

    # ğŸ›¡ï¸ å¢å¼ºï¼šæ·»åŠ æµè§ˆå™¨ Fetch å¤´ï¼Œä¼ªè£…æˆçœŸå®çš„è§†é¢‘æ’­æ”¾è¯·æ±‚
    headers.update({
        "Sec-Fetch-Dest": "video",
        "Sec-Fetch-Mode": "no-cors",
        "Sec-Fetch-Site": "cross-site",
        "Accept-Encoding": "identity",  # è§†é¢‘æµä¸éœ€è¦ gzip
        "Connection": "keep-alive"
    })

    chain = []
    if pool_manager: chain = pool_manager.get_standard_chain()
    chain.append((None, "Direct", 10))

    for proxy_url, name, timeout_sec in chain:
        try:
            proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
            loop = asyncio.get_running_loop()

            # ä½¿ç”¨ requests è·å–æµï¼Œä¸è¯»å–å†…å®¹ï¼Œåªè·å– headers å’Œ iterator
            resp = await loop.run_in_executor(None, lambda: requests.get(
                target_url, headers=headers, cookies=req_cookies,
                stream=True, timeout=(5, timeout_sec), verify=False, proxies=proxies
            ))

            if resp.status_code >= 400: continue

            content_type = resp.headers.get("content-type", "application/octet-stream")

            # å¦‚æœæ˜¯ m3u8 æ’­æ”¾åˆ—è¡¨ï¼Œéœ€è¦é‡å†™å†…éƒ¨çš„ TS é“¾æ¥
            if "mpegurl" in content_type or ".m3u8" in target_url:
                text = resp.text # è¿™é‡Œä¼šè¯»å–å†…å®¹ï¼Œä½† m3u8 é€šå¸¸å¾ˆå°
                new_lines = []
                for line in text.splitlines():
                    if line and not line.startswith("#"):
                        full_ts = urljoin(target_url, line.strip())
                        line = f"http://127.0.0.1:8000/api/proxy?url={quote(full_ts)}"
                    new_lines.append(line)
                return Response(content="\n".join(new_lines), media_type=content_type)

            # æµå¼ä¼ è¾“å†…å®¹
            def iter_content():
                for chunk in resp.iter_content(chunk_size=64 * 1024):
                    yield chunk

            return StreamingResponse(
                iter_content(),
                status_code=resp.status_code,
                headers={
                    "Content-Type": content_type,
                    "Content-Range": resp.headers.get("Content-Range"),
                    "Content-Length": resp.headers.get("Content-Length"),
                    "Accept-Ranges": "bytes"
                },
                media_type=content_type
            )
        except:
            continue

    return Response(status_code=502, content="Stream Failed")


@router.post("/api/crawl")
async def start_crawl(request: CrawlRequest):
    return StreamingResponse(smart_router(request.url, request.mode), media_type="application/x-ndjson")


@router.get("/download/{filename}")
async def download_file(filename: str):
    filepath = os.path.abspath(filename)
    if os.path.exists(filepath): return FileResponse(filepath, filename=filename)
    return Response("File not found", status_code=404)
